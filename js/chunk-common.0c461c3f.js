(globalThis["webpackChunkquasar_tufte"]=globalThis["webpackChunkquasar_tufte"]||[]).push([[64],{98673:(e,n,t)=>{"use strict";t.d(n,{Z:()=>a});const a="@inproceedings{10.1007/978-3-030-02465-9_2,\n  title = {Cost and {{Performance Modeling}} for {{Earth System Data Management}} and {{Beyond}}},\n  booktitle = {High {{Performance Computing}}},\n  author = {L{\\\"u}ttgau, Jakob and Kunkel, Julian},\n  editor = {Yokota, Rio and Weiland, Mich{\\`e}le and Shalf, John and Alam, Sadaf},\n  year = {2018},\n  series = {Lecture {{Notes}} in {{Computer Science}}},\n  pages = {23--35},\n  publisher = {Springer Int. Publishing},\n  address = {Cham},\n  doi = {10.1007/978-3-030-02465-9_2},\n  abstract = {Current and anticipated storage environments confront domain scientist and data center operators with usability, performance and cost challenges. The amount of data upcoming system will be required to handle is expected to grow exponentially, mainly due to increasing resolution and affordable compute power. Unfortunately, the relationship between cost and performance is not always well understood requiring considerable effort for educated procurement. Within the Centre of Excellence in Simulation of Weather and Climate in Europe (ESiWACE) models to better understand cost and performance of current and future systems are being explored. This paper presents models and methodology focusing on, but not limited to, data centers used in the context of climate and numerical weather prediction. The paper concludes with a case study of alternative deployment strategies and outlines the challenges anticipating their impact on cost and performance. By publishing these early results, we would like to make the case to work towards standard models and methodologies collaboratively as a community to create sufficient incentives for vendors to provide specifications in formats which are compatible to these modeling tools. In addition to that, we see application for such formalized models and information in I/O related middleware, which are expected to make automated but reasonable decisions in increasingly heterogeneous data centers.},\n  isbn = {978-3-030-02465-9},\n  langid = {english},\n  keywords = {Cost,Data management,Earth systems,Storage,TCO}\n}\n\n@inproceedings{10.1007/978-3-319-67630-2_12,\n  title = {Simulation of {{Hierarchical Storage Systems}} for {{TCO}} and {{QoS}}},\n  booktitle = {High {{Performance Computing}}},\n  author = {Luettgau, Jakob and Kunkel, Julian},\n  editor = {Kunkel, Julian M. and Yokota, Rio and Taufer, Michela and Shalf, John},\n  year = {2017},\n  series = {Lecture {{Notes}} in {{Computer Science}}},\n  pages = {132--144},\n  publisher = {Springer International Publishing},\n  address = {Cham},\n  doi = {10.1007/978-3-319-67630-2_12},\n  abstract = {Due to the variety of storage technologies deep storage hierarchies turn out to be the most feasible choice to meet performance and cost requirements when handling vast amounts of data. Long-term archives employed by scientific users are mainly reliant on tape storage, as it remains the most cost-efficient option. Archival systems are often loosely integrated into the HPC storage infrastructure. In expectation of exascale systems and in situ analysis also burst buffers will require integration with the archive. Exploring new strategies and developing open software for tape systems is a hurdle due to the lack of affordable storage silos and availability outside of large organizations and due to increased wariness requirements when dealing with ultra-durable data. Lessening these problems by providing virtual storage silos should enable community-driven innovation and enable site operators to add features where they see fit while being able to verify strategies before deploying on production systems. Different models for the individual components in tape systems are developed. The models are then implemented in a prototype simulation using discrete event simulation. The work shows that the simulations can be used to approximate the behavior of tape systems deployed in the real world and to conduct experiments without requiring a physical tape system.},\n  isbn = {978-3-319-67630-2},\n  langid = {english},\n  keywords = {Hierarchical storage systems,Long-term archive,Modeling,Performance,Simulation,Tape,Total cost of ownership}\n}\n\n@techreport{10.5281/zenodo.1228749,\n  title = {Business {{Model}} with {{Alternative Scenarios}} ({{D4}}.1)},\n  author = {Luettgau, Jakob and Kunkel, Julian and Jensen, Jens and Lawrence, Bryan},\n  year = {2017},\n  month = feb,\n  institution = {{Centre of Excellence for Weather and Climate in Europe (ESiWACE)}},\n  doi = {10.5281/zenodo.1228749},\n  url = {https://zenodo.org/record/1228749},\n  urldate = {2022-07-30},\n  abstract = {Abstract This report summarizes the work on requirements and business models for the storage infrastructure within weather and climate data centres (although much of the work has wider applicability).~ The report concentrates on identifying and evaluating the interplay of important cost factors, along with an introduction to relevant (storage and data movement) hardware and software technology,~ terminology and performance metrics. The report begins with a description of how climate and weather applications make use of HPC systems, the arising challenges and data requirements, and some trends that are likely to impact future data centre designs. Related work follows that introduces some cost developments, cost modelling and technological developments. The body of the work is an integrated graph-based approach to modelling costs, resilience and performance for storage systems. Storage models are evaluated in several scenarios each introducing some architectural changes to currently deployed high-performance systems and discusses the cost and performance implications. These discussions are made on a high-level of abstraction as no model is able to predict the non-linear behaviour when scaling out big systems accurately. Conclusions identify the potential benefits that more refined models might offer and outline future work. ~ About this document Work package in charge: WP4 Exploitability Actual delivery date for this deliverable: 24 February 2017 Dissemination level: PU (for public use) Lead author:~ Deutsches Klimarechenzentrum (DKRZ), ~Jakob Luettgau Other contributing authors: Deutsches Klimarechenzentrum (DKRZ),~ Julian Kunkel, Jakob Luettgau Science and Technology Facilities Council (STFC), Jens Jensen The University of Reading (UREAD), and Science and Technology Facilities Council (STFC), Bryan Lawrence},\n  langid = {english},\n  keywords = {business models,climate data centre,data centre design,HPC,performance implications.,storage and data movement,storage infrastructure}\n}\n\n@techreport{10.5281/zenodo.1453895,\n  title = {Operational {{Demonstrator}} of {{ESD Middleware}} ({{MS5}})},\n  author = {Kunkel, Julian and Luettgau, Jakob and Betke, Eugen},\n  year = {2017},\n  month = aug,\n  institution = {{Centre of Excellence for Weather and Climate in Europe (ESiWACE)}},\n  doi = {10.5281/zenodo.1453895},\n  url = {https://zenodo.org/record/1453895},\n  urldate = {2022-07-30},\n  abstract = {The demonstrator consists of three parts: 1) A typical I/O workload that is mimicking checkpoint/restart, 2) a HDF5 VOL Plugin to redirect and persist data to different storage targets and 3) a policy engine that fetches runtime information and adapts the storage target accordingly.},\n  langid = {english}\n}\n\n@techreport{10.5281/zenodo.1453902,\n  title = {Prototypes of {{Alternative Storage Backends}} ({{MS7}})},\n  author = {Kunkel, Julian and Luettgau, Jakob},\n  year = {2018},\n  month = may,\n  institution = {{Centre of Excellence for Weather and Climate in Europe (ESiWACE)}},\n  doi = {10.5281/zenodo.1453902},\n  url = {https://zenodo.org/record/1453902},\n  urldate = {2022-07-30},\n  abstract = {The objectives of the Exploitability work package within ESiWACE include two objectives: (1) The development of a new way of laying data onto storage which provides performance without compromising the familiar NetCDF interface. (2) The development of a prototype tape library. We are delivering these two objectives using the same basic philosophy, which is to develop middleware which is easily deployable by users and system administrators, and which utilises fragmentation to provide performance. The current design includes the same basic concepts in two different software packages (the ``Earth System Data Middleware'' and the ``Semantic Storage Layer'') which target these two objectives. Future projects may merge some or all the underlying functionality. This milestone (7) reports a staging point in the development of these two components, the release of tagged software for the ESDM and SemSL.}\n}\n\n@techreport{10.5281/zenodo.2573896,\n  title = {New {{Storage Layout}} for {{Earth System Data}} ({{D4}}.2)},\n  author = {Luettgau, Jakob and Kunkel, Julian and Lawrence, Bryan and Jensen, Jens and Congiu, {\\relax Gi}useppe and Hua, Huang and Nassisi, Paola},\n  year = {2017},\n  month = jul,\n  institution = {{Centre of Excellence for Weather and Climate in Europe (ESiWACE)}},\n  doi = {10.5281/zenodo.2573896},\n  url = {https://zenodo.org/record/2573896},\n  urldate = {2022-07-30},\n  abstract = {Abstract Making the best use of HPC in Earth simulation requires storing and manipulating vast quantities of data. Existing storage environments face usability and performance challenges for both domain scientists and the data centers supporting the scientists. These challenges arise from data discovery/access patterns, and the need to support complex legacy interfaces. In the ESiWACE project, we develop a novel I/O middleware targeting, but not limited to, earth system data. This deliverable sheds light upon the technical design of the ESD middleware, and the user perspective and implications when using the middleware. Its architecture builds on well-established end-user interfaces but utilizes scientific metadata to harness a data structure centric perspective. In contrast to existing solutions, the middleware maps data structures to available storage technology based on several parameters: 1) A data center specific configuration of available hardware with their characteristics; 2) The intended usage pattern explicitly provided by the user and implicitly by the structure of the data. This allows to exploit performance characteristics of a heterogeneous storage environment more efficiently. This deliverable provides the background on data representations and description formats commonly used in earth system modeling. The document isolates the key requirements for an earth system middleware and collects numerous use-case outlining the benefit to existing and anticipated workflows and technologies. Finally, a detailed initial design for the architecture of the earth system middleware is proposed and documented. The document is not intended to describe all components completely but provides a high-level overview that is necessary to build a first prototype as it is planned in the next phase of the ESiWACE project. During this development, the design will be adjusted to match the prototype; the final version of the design document will be delivered with the end of the project. ~ About this document Work package in charge: WP4 Exploitability Actual delivery date for this deliverable: July 2017 Dissemination level: PU Lead author:~ Deutsches Klimarechenzentrum GmbH (DKRZ), Jakob L{\\\"u}ttgau Other contributing authors: Deutsches Klimarechenzentrum GmbH (DKRZ), Julian Kunkel Science and Technology Facilities Council (STFC), Bryan Lawrence, Jens Jensen The University of Reading (UREAD), Bryan Lawrence Seagate Systems UK Limited (SEAGATE), Giuseppe Congiu, Huang Hua Fondazione Centro Euro-Mediterraneo sui Cambiamenti Climatici (CMCC): Paola Nassisi},\n  langid = {english},\n  keywords = {Earth simulation,earth system data,HPC,I/O middleware,storage environments}\n}\n\n@techreport{10.5281/zenodo.3551787,\n  title = {New {{Storage Layout}} for {{Earth System Data}} ({{D4}}.2) ({{Version}} 2.0)},\n  author = {Luettgau, Jakob and Kunkel, Julian and Lawrence, Bryan and Jensen, Jens and Congiu, Giuseppe and Hua, Huan and Paola, Nassisi},\n  year = {2019},\n  month = nov,\n  institution = {{Centre of Excellence for Weather and Climate in Europe (ESiWACE)}},\n  doi = {10.5281/zenodo.3551787},\n  url = {https://zenodo.org/record/3551787},\n  urldate = {2022-07-30},\n  abstract = {Making the best use of HPC in Earth simulation requires storing and manipulating vast quantities of data. Existing storage environments face usability and performance challenges for both domain scientists and the data centers supporting the scientists. These challenges arise from data discovery/access patterns, and the need to support complex legacy interfaces. In the ESiWACE project, we develop a novel I/O middleware targeting, but not limited to, earth system data. This deliverable sheds light upon the technical design of the ESD middleware, and the user perspective and implications when using the middleware. Its architecture builds on well-established end-user interfaces but utilizes scientific metadata to harness a data structure centric perspective. In contrast to existing solutions, the middleware maps data structures to available storage technology based on several parameters: 1) A data center specific configuration of available hardware with their characteristics; 2) The intended usage pattern explicitly provided by the user and implicitly by the structure of the data. This allows to exploit performance characteristics of a heterogeneous storage environment more efficiently. This deliverable provides the background on data representations and description formats commonly used in earth system modeling. The document isolates the key requirements for an earth system middleware and collects numerous use-case outlining the benefit to existing and anticipated workflows and technologies. Finally, a detailed initial design for the architecture of the earth system middleware is proposed and documented. The document is not intended to describe all components completely but provides a high-level overview that is necessary to build a first prototype as it is planned in the next phase of the ESiWACE project. During this development, the design will be adjusted to match the prototype; the final version of the design document will be delivered with the end of the project.},\n  langid = {english},\n  keywords = {HPC Earth simulation storage environments I/O middleware earth system data}\n}\n\n@inproceedings{ATSFNAHLBP17,\n  title = {Adaptive {{Tier Selection}} for {{NetCDF}} and {{HDF5}}},\n  booktitle = {Supercomputing {{Conference}} 2017 ({{SC17}})},\n  author = {Luettgau, Jakob and Betke, Eugen and Perevalova, Olga and Kunkel, Julian and Kuhn, Michael},\n  year = {2017},\n  month = nov,\n  address = {Denver, CO, USA}\n}\n\n@inproceedings{kunkelMiddlewareEarthSystem2016,\n  title = {Middleware for {{Earth System Data}}},\n  author = {Kunkel, Julian and Luettgau, Jakob and Lawrence, Bryan N and Jensen, Jens and Congiu, Giuseppe and Readey, John},\n  year = {2016},\n  publisher = {1st Joint International Workshop on Parallel Data Storage \\& Data Intensive Scalable Computing Systems (PDSW-DISCS) WIPs},\n  abstract = {Making the best use of HPC in Earth simulation requires storing and manipulating vast quantities of data. Existing storage environments face usability and performance challenges for both domain scientists and the data centers supporting the scientists. These challenges arise from data discovery/access patterns, and the need to support complex legacy interfaces.},\n  langid = {english}\n}\n\n@inproceedings{MASOTLFHSS16,\n  title = {Modeling and {{Simulation}} of {{Tape Libraries}} for {{Hierarchical Storage Systems}}},\n  booktitle = {Supercomputing {{Computing}}  ({{SC16}})},\n  author = {L{\\\"u}ttgau, Jakob and Kunkel, Julian},\n  year = {2016},\n  month = nov,\n  address = {Salt Lake City, Utah, USA},\n  url = {http://sc16.supercomputing.org/sc-archive/tech_poster/tech_poster_pages/post123.html},\n  activity = {SC16}\n}\n"},99576:(e,n,t)=>{"use strict";t.d(n,{Z:()=>a});const a='@book{MachineLearningMarine2021,\n  title = {Machine {{Learning}} for {{Marine Sciences}}, {{Part}} 2: {{Data}} Preprocessing},\n  year = {2021},\n  month = sep,\n  series = {{{MarDATA Courses}} for {{Doctoral Students}}},\n  publisher = {HelmholtzAI}\n}\n\n@book{MachineLearningMarine2021a,\n  title = {Machine {{Learning}} for {{Marine Sciences}}, {{Part}} 1: {{Introduction}}},\n  year = {2021},\n  month = sep,\n  series = {{{MarDATA Courses}} for {{Doctoral Students}}},\n  publisher = {HelmholtzAI}\n}\n\n@inproceedings{weigelAIEarthSystem2021,\n  title = {{{AI}} for {{Earth System Sciences}}: {{Bottlenecks}}, {{Pitfalls}} and {{Recommendations}}},\n  booktitle = {Platform for {{Advanced Scientific Computing}} ({{PASC21}})},\n  author = {Weigel, Tobias and L{\\"u}ttgau, Jakob and Kadow, Christopher and Greenberg, David and Bouwer, Laurens M. and Bockelmann, Hendryk and Schrum, Corinna and Ludwig, Thomas},\n  year = {2021},\n  month = jul,\n  publisher = {PASC21},\n  url = {https://pasc21.pasc-conference.org/program/schedule/index.html%3Fpost_type=page&p=10&id=post143&sess=sess182.html},\n  isbn = {Geneva, Switzerland}\n}\n'},41081:(e,n,t)=>{"use strict";t.d(n,{Z:()=>a});const a="@inproceedings{10.1109/PDSW-DISCS.2018.00012,\n  title = {Toward {{Understanding I}}/{{O Behavior}} in {{HPC Workflows}}},\n  booktitle = {2018 {{IEEE}}/{{ACM}} 3rd {{International Workshop}} on {{Parallel Data Storage}} \\& {{Data Intensive Scalable Computing Systems}} ({{PDSW-DISCS}})},\n  author = {Luettgau, Jakob and Snyder, Shane and Carns, Philip and Wozniak, Justin M. and Kunkel, Julian and Ludwig, Thomas},\n  year = {2018},\n  month = nov,\n  pages = {64--75},\n  address = {Dallas, TX, USA},\n  doi = {10.1109/PDSW-DISCS.2018.00012},\n  abstract = {Scientific discovery increasingly depends on complex workflows consisting of multiple phases and sometimes millions of parallelizable tasks or pipelines. These workflows access storage resources for a variety of purposes, including preprocessing, simulation output, and postprocessing steps. Unfortunately, most workflow models focus on the scheduling and allocation of computational resources for tasks while the impact on storage systems remains a secondary objective and an open research question. I/O performance is not usually accounted for in workflow telemetry reported to users. In this paper, we present an approach to augment the I/O efficiency of the individual tasks of workflows by combining workflow description frameworks with system I/O telemetry data. A conceptual architecture and a prototype implementation for HPC data center deployments are introduced. We also identify and discuss challenges that will need to be addressed by workflow management and monitoring systems for HPC in the future. We demonstrate how real-world applications and workflows could benefit from the approach, and we show how the approach helps communicate performance-tuning guidance to users.},\n  keywords = {Data models,Engines,HPC,I/O,Instrumentation,Monitoring,Pipelines,Storage,Task analysis,Telemetry,Tools,Visualization,Workflow}\n}\n\n@inproceedings{10.1145/3624062.3624207,\n  title = {Enabling {{Agile Analysis}} of {{I}}/{{O Performance Data}} with {{PyDarshan}}},\n  booktitle = {Proceedings of the {{SC}} '23 {{Workshops}} of {{The International Conference}} on {{High Performance Computing}}, {{Network}}, {{Storage}}, and {{Analysis}}},\n  author = {Luettgau, Jakob and Snyder, Shane and Reddy, Tyler and Awtrey, Nikolaus and Harms, Kevin and Bez, Jean Luca and Wang, Rui and Latham, Rob and Carns, Philip},\n  year = {2023},\n  month = nov,\n  series = {{{SC-W}} '23},\n  pages = {1380--1391},\n  publisher = {Association for Computing Machinery},\n  address = {New York, NY, USA},\n  doi = {10.1145/3624062.3624207},\n  url = {https://doi.org/10.1145/3624062.3624207},\n  urldate = {2023-12-02},\n  abstract = {Modern scientific applications utilize numerous software and hardware layers to efficiently access data. This approach poses a challenge for I/O optimization because of the need to instrument and correlate information across those layers. The Darshan characterization tool seeks to address this challenge by providing efficient, transparent, and compact runtime instrumentation of many common I/O interfaces. It also includes command-line tools to generate actionable insights and summary reports. However, the extreme diversity of today's scientific applications means that not all applications are well served by one-size-fits-all analysis tools. In this work we present PyDarshan, a Python-based library that enables agile analysis of I/O performance data. PyDarshan caters to both novice and advanced users by offering ready-to-use HTML reports as well as a rich collection of APIs to facilitate custom analyses. We present the design of PyDarshan and demonstrate its effectiveness in four diverse real-world analysis use cases.},\n  isbn = {979-8-4007-0785-8},\n  keywords = {High-Performance Computing,Input/Output,Performance Analysis,Storage}\n}\n\n@phdthesis{luttgauDecisionSupportWorkflowAware2021,\n  type = {Dissertation},\n  title = {Decision {{Support}} for {{Workflow-Aware High-Performance Storage Systems}}},\n  author = {L{\\\"u}ttgau, Jakob},\n  year = {2021},\n  month = may,\n  address = {Hamburg},\n  url = {https://jakobluettgau.com/static/theses/luettgau_decision-support-for-workflow-aware-high-performance-storage-systems.pdf},\n  langid = {english},\n  school = {Universit{\\\"a}t Hamburg}\n}\n"},17926:(e,n,t)=>{"use strict";t.d(n,{Z:()=>a});const a="@inproceedings{10.1109/eScience55777.2022.00068,\n  title = {Ubique: {{A New Model}} for {{Untangling Inter-task Data Dependence}} in {{Complex HPC Workflows}}},\n  shorttitle = {Ubique},\n  booktitle = {2022 {{IEEE}} 18th {{International Conference}} on E-{{Science}} (e-{{Science}})},\n  author = {Yeom, Jae-Seung and Ahn, Dong H. and Lumsden, Ian and Luettgau, Jakob and {Caino-Lores}, Silvina and Taufer, Michela},\n  year = {2022},\n  month = oct,\n  pages = {421--422},\n  doi = {10.1109/eScience55777.2022.00068},\n  url = {https://ieeexplore.ieee.org/document/9973591},\n  urldate = {2025-01-05},\n  abstract = {Exploiting task parallelism is getting increasingly difficult for diverse and complex scientific workflows running on High Performance Computing (HPC) systems. In this paper, we argue that the difficulty rises from a void in the spectrum of existing data-transfer models for resolving inter-task data dependence within a workflow and propose a novel model to fill that gap: Ubique. The Ubique model combines the best from in-transit and in situ models in order for loosely coupled producer and consumer tasks to run concurrently and to resolve their data dependencies efficiently with little or no modifications to their codes, striking a balance between transparent optimization, productivity, and performance. Our preliminary evaluation suggests that Ubique can significantly outperform the parallel file system (PFS)-based model while offering automatic data transfer and synchronization which are the features lacking in many traditional models. It also identifies the performance characteristics of its key depending subsystems, which must be understood for further broadening its benefits.},\n  keywords = {/unread,Computational modeling,Data models,Data transfer,File systems,High performance computing,in situ,in transit,job scheduling,Parallel processing,Productivity,workflow}\n}\n"},50389:(e,n,t)=>{"use strict";t.d(n,{Z:()=>a});const a="@inproceedings{10.1109/eScience55777.2022.00039,\n  title = {Enabling {{Call Path Querying}} in {{Hatchet}} to {{Identify Performance Bottlenecks}} in {{Scientific Applications}}},\n  booktitle = {2022 {{IEEE}} 18th {{International Conference}} on E-{{Science}} (e-{{Science}})},\n  author = {Lumsden, Ian and Luettgau, Jakob and Lama, Vanessa and {Scully-Allison}, Connor and Brink, Stephanie and Isaacs, Katherine E. and Pearce, Olga and Taufer, Michela},\n  year = {2022},\n  month = oct,\n  pages = {256--266},\n  doi = {10.1109/eScience55777.2022.00039},\n  url = {https://doi.org/10.1109/eScience55777.2022.00039},\n  urldate = {2024-02-01},\n  abstract = {As computational science applications benefit from larger-scale, more heterogeneous high performance computing (HPC) systems, the process of studying their performance becomes increasingly complex. The performance data analysis library Hatchet provides some insights into this complexity, but is currently limited in its analysis capabilities. Missing capabilities include the handling of relational caller-callee data captured by HPC profilers. To address this shortcoming, we augment Hatchet with a Call Path Query Language that leverages relational data in the performance analysis of scientific applications. Specifically, our Query Language enables data reduction using call path pattern matching. We demonstrate the effectiveness of our Query Language in identifying performance bottlenecks and enhancing Hatchet's analysis capabilities through three case studies. In the first case study, we compare the performance of sequential and multi-threaded versions of the graph alignment application Fido. In doing so, we identify the existence of large memory inefficiencies in both versions. In the second case study, we examine the performance of MPI calls in the linear algebra mini-application AMG2013 when using MVAPICH and Spectrum-MPI. In doing so, we identify hidden performance losses in specific MPI functions. In the third case study, we illustrate the use of our Query Language in Hatchet's interactive visualization. In doing so, we show that our Query Language enables a simple and intuitive way to massively reduce profiling data.},\n  keywords = {Full stack,High performance computing,High Performance Computing,Libraries,Linear algebra,Message Passing,Performance analysis,Performance Analysis,Proteins,Query Language,Scientific Applications,Scientific computing}\n}\n"},20165:(e,n,t)=>{"use strict";t.d(n,{Z:()=>a});const a="@inproceedings{10.1145/3588195.3592989,\n  title = {Thicket: {{Seeing}} the Performance Experiment Forest for the Individual Run Trees},\n  booktitle = {Proceedings of the 32nd International {{ACM}} Symposium on High-Performance Parallel and Distributed Computing ({{HPDC}})},\n  author = {Brink, Stephanie and McKinsey, Michael and Boehme, David and Hawkins, W. Daryl and {Scully-Allison}, Connor and Lumsden, Ian and Burgess, Treece and Lama, Vanessa and Isaacs, Katherine E. and Luettgau, Jakob and {Michela Taufer} and Pearce, Olga},\n  year = {2023},\n  month = jun,\n  pages = {1--10},\n  publisher = {ACM},\n  address = {Orlando, Florida, USA},\n  doi = {10.1145/3588195.3592989},\n  url = {https://doi.org/10.1145/3588195.3592989}\n}\n"},46476:(e,n,t)=>{"use strict";t.d(n,{Z:()=>a});const a="@inproceedings{10.1007/978-3-031-34668-2_25,\n  ids = {Jakob_hci23},\n  title = {Development of~{{Large-Scale Scientific Cyberinfrastructure}} and~the~{{Growing Opportunity}} to~{{Democratize Access}} to~{{Platforms}} and~{{Data}}},\n  booktitle = {Distributed, {{Ambient}} and {{Pervasive Interactions}}},\n  author = {Luettgau, Jakob and Scorzelli, Giorgio and Pascucci, Valerio and Taufer, Michela},\n  editor = {Streitz, Norbert A. and Konomi, Shin'ichi},\n  year = {2023},\n  series = {Lecture {{Notes}} in {{Computer Science}}},\n  pages = {378--389},\n  publisher = {Springer Nature Switzerland},\n  address = {Cham},\n  doi = {10.1007/978-3-031-34668-2_25},\n  url = {https://doi.org/10.1007/978-3-031-34668-2_25},\n  abstract = {As researchers across scientific domains rapidly adopt advanced scientific computing methodologies, access to advanced cyberinfrastructure (CI) becomes a critical requirement in scientific discovery. Lowering the entry barriers to CI is a crucial challenge in interdisciplinary sciences requiring frictionless software integration, data sharing from many distributed sites, and access to heterogeneous computing platforms. In this paper, we explore how the challenge is not merely a factor of availability and affordability of computing, network, and storage technologies but rather the result of insufficient interfaces with an increasingly heterogeneous mix of computing technologies and data sources. With more distributed computation and data, scientists, educators, and students must invest their time and effort in coordinating data access and movements, often penalizing their scientific research. Investments in the interfaces' software stack are necessary to help scientists, educators, and students across domains take advantage of advanced computational methods. To this end, we propose developing a science data fabric as the standard scientific discovery interface that seamlessly manages data dependencies within scientific workflows and CI.},\n  isbn = {978-3-031-34668-2},\n  langid = {english},\n  keywords = {Cyberinfrastructure,National Science Data Fabric,Scientific data}\n}\n\n@inproceedings{10.1109/UCC56403.2022.00011,\n  ids = {luettgauNSDFCatalogLightweightIndexing2022},\n  title = {{{NSDF-Catalog}}: {{Lightweight Indexing Service}} for {{Democratizing Data Delivery}}},\n  shorttitle = {{{NSDF-Catalog}}},\n  booktitle = {2022 {{IEEE}}/{{ACM}} 15th {{International Conference}} on {{Utility}} and {{Cloud Computing}} ({{UCC}})},\n  author = {Luettgau, Jakob and Scorzelli, Giorgio and Pascucci, Valerio and Tarcea, Glenn and Kirkpatrick, Christine R. and Taufer, Michela},\n  year = {2022},\n  month = dec,\n  pages = {1--10},\n  address = {Portland, Oregon, USA},\n  doi = {10.1109/UCC56403.2022.00011},\n  url = {https://doi.org/10.1109/UCC56403.2022.00011},\n  urldate = {2024-01-22},\n  abstract = {Across domains massive amounts of scientific data are generated. Because of the large volume of information, data discoverability is a challenge, especially for scientists who have not generated the data or are from other domains. As part of the NSF-funded National Science Data Fabric (NSDF) initiative, we developed a testbed to demonstrate that these boundaries to data discoverability can be overcome. In support of this effort, we identify the need for indexing large-amounts of scientific data across scientific domains. We propose NSDF-Catalog, a lightweight indexing service with minimal metadata that complements existing domain-specific and rich-metadata col-lections. NSDF-Catalog is designed to facilitate multiple related objectives within a flexible microservice to: (i) coordinate data movements and replication of data from origin repositories within the NSDF federation; (ii) build an inventory of existing scientific data to inform the design of next-generation cyberinfrastructure; and (iii) provide a suite of tools for discovery of datasets for cross-disciplinary research. Our service indexes scientific data at a fine-granularity at the file or object level to inform data distribution strategies and to improve the experience for users from the consumer perspective, with the goal of allowing end-to-end dataflow optimizations.}\n}\n\n@inproceedings{10.1145/3502181.3533709,\n  ids = {olayaNSDFFUSETestbedStudying2022},\n  title = {{{NSDF-FUSE}}: {{A Testbed}} for {{Studying Object Storage}} via {{FUSE File Systems}}},\n  shorttitle = {{{NSDF-FUSE}}},\n  booktitle = {Proceedings of the 31st {{International Symposium}} on {{High-Performance Parallel}} and {{Distributed Computing}}},\n  author = {Olaya, Paula and Luettgau, Jakob and Zhou, Naweiluo and Lofstead, Jay and Scorzelli, Giorgio and Pascucci, Valerio and Taufer, Michela},\n  year = {2022},\n  month = jun,\n  series = {{{HPDC}} '22},\n  pages = {277--278},\n  publisher = {Association for Computing Machinery},\n  address = {New York, NY, USA},\n  doi = {10.1145/3502181.3533709},\n  url = {https://doi.org/10.1145/3502181.3533709},\n  urldate = {2022-07-11},\n  abstract = {This work presents NSDF-FUSE, a testbed for evaluating settings and performance of FUSE-based file systems on top of S3-compatible object storage; the testbed is part of a suite of services from the National Science Data Fabric (NSDF) project (an NSF-funded project that is delivering cyberinfrastructures for data scientists). We demonstrate how NSDF-FUSE can be deployed to evaluate eight different mapping packages that mount S3-compatible object storage to a file system, as well as six data patterns representing different I/O operations on two cloud platforms. NSDF-FUSE is open-source and can be easily extended to run with other software mapping packages and different cloud platforms.},\n  isbn = {978-1-4503-9199-3},\n  keywords = {cloud,file system,fuse,object storage,performance}\n}\n\n@inproceedings{10.1145/3502181.3533710,\n  ids = {luettgauNSDFCloudEnablingAdHoc2022a,luettgauNSDFCloudEnablingAdHoc2022b,luettgauNSDFCloudEnablingAdHoc2022c},\n  title = {{{NSDF-Cloud}}: {{Enabling Ad-Hoc Compute Clusters Across Academic}} and {{Commercial Clouds}}},\n  shorttitle = {{{NSDF-Cloud}}},\n  booktitle = {Proceedings of the 31st {{International Symposium}} on {{High-Performance Parallel}} and {{Distributed Computing}}},\n  author = {Luettgau, Jakob and Olaya, Paula and Zhou, Naweiluo and Scorzelli, Giorgio and Pascucci, Valerio and Taufer, Michela},\n  year = {2022},\n  month = jun,\n  pages = {279--280},\n  publisher = {ACM},\n  address = {Minneapolis MN USA},\n  doi = {10.1145/3502181.3533710},\n  url = {https://dl.acm.org/doi/10.1145/3502181.3533710},\n  urldate = {2023-02-15},\n  isbn = {978-1-4503-9199-3},\n  langid = {english},\n  keywords = {cyberinfrastructure,data fabric,virtual machine}\n}\n\n@article{10.1177/10943420231167800,\n  ids = {zhouOrchestrationMaterialsScience2023a},\n  title = {Orchestration of Materials Science Workflows for Heterogeneous Resources at Large Scale},\n  author = {Zhou, Naweiluo and Scorzelli, Giorgio and Luettgau, Jakob and Kancharla, Rahul R and Kane, Joshua J and Wheeler, Robert and Croom, Brendan P and Newell, Pania and Pascucci, Valerio and Taufer, Michela},\n  year = {2023},\n  month = jul,\n  journal = {The International Journal of High Performance Computing Applications},\n  volume = {37},\n  number = {3-4},\n  pages = {260--271},\n  publisher = {SAGE Publications Ltd STM},\n  issn = {1094-3420},\n  doi = {10.1177/10943420231167800},\n  url = {https://doi.org/10.1177/10943420231167800},\n  urldate = {2024-01-22},\n  abstract = {In the era of big data, materials science workflows need to handle large-scale data distribution, storage, and computation. Any of these areas can become a performance bottleneck. We present a framework for analyzing internal material structures (e.g., cracks) to mitigate these bottlenecks. We demonstrate the effectiveness of our framework for a workflow performing synchrotron X-ray computed tomography reconstruction and segmentation of a silica-based structure. Our framework provides a cloud-based, cutting-edge solution to challenges such as growing intermediate and output data and heavy resource demands during image reconstruction and segmentation. Specifically, our framework efficiently manages data storage, scaling up compute resources on the cloud. The multi-layer software structure of our framework includes three layers. A top layer uses Jupyter notebooks and serves as the user interface. A middle layer uses Ansible for resource deployment and managing the execution environment. A low layer is dedicated to resource management and provides resource management and job scheduling on heterogeneous nodes (i.e., GPU and CPU). At the core of this layer, Kubernetes supports resource management, and Dask enables large-scale job scheduling for heterogeneous resources. The broader impact of our work is four-fold: through our framework, we hide the complexity of the cloud's software stack to the user who otherwise is required to have expertise in cloud technologies; we manage job scheduling efficiently and in a scalable manner; we enable resource elasticity and workflow orchestration at a large scale; and we facilitate moving the study of nonporous structures, which has wide applications in engineering and scientific fields, to the cloud. While we demonstrate the capability of our framework for a specific materials science application, it can be adapted for other applications and domains because of its modular, multi-layer architecture.},\n  langid = {english}\n}\n\n@inproceedings{zhou_escience22,\n  title = {A Software Framework for Scientific Workflow Orchestration at Large Scale},\n  booktitle = {Proceedings of the 18th {{IEEE}} International Conference on E-{{Science}} ({{eScience}})},\n  author = {Zhou, Nauweiluo and Luettgau, Jakob and Kancharla, Rahul Reddy and Kane, Joshua and Croom, Brendan and Wheeler, Robert and Newell, Pania and Scorzelli, Giorgio and Pascucci, Valerio and Taufer, Michela},\n  year = {2022},\n  month = oct,\n  pages = {1--1},\n  publisher = {IEEE Computer Society},\n  address = {Salt Lake City, Utah, USA}\n}\n"},92819:(e,n,t)=>{"use strict";t.d(n,{Z:()=>a});const a="@inproceedings{10.1145/3624062.3624207,\n  title = {Enabling {{Agile Analysis}} of {{I}}/{{O Performance Data}} with {{PyDarshan}}},\n  booktitle = {Proceedings of the {{SC}} '23 {{Workshops}} of {{The International Conference}} on {{High Performance Computing}}, {{Network}}, {{Storage}}, and {{Analysis}}},\n  author = {Luettgau, Jakob and Snyder, Shane and Reddy, Tyler and Awtrey, Nikolaus and Harms, Kevin and Bez, Jean Luca and Wang, Rui and Latham, Rob and Carns, Philip},\n  year = {2023},\n  month = nov,\n  series = {{{SC-W}} '23},\n  pages = {1380--1391},\n  publisher = {Association for Computing Machinery},\n  address = {New York, NY, USA},\n  doi = {10.1145/3624062.3624207},\n  url = {https://doi.org/10.1145/3624062.3624207},\n  urldate = {2023-12-02},\n  abstract = {Modern scientific applications utilize numerous software and hardware layers to efficiently access data. This approach poses a challenge for I/O optimization because of the need to instrument and correlate information across those layers. The Darshan characterization tool seeks to address this challenge by providing efficient, transparent, and compact runtime instrumentation of many common I/O interfaces. It also includes command-line tools to generate actionable insights and summary reports. However, the extreme diversity of today's scientific applications means that not all applications are well served by one-size-fits-all analysis tools. In this work we present PyDarshan, a Python-based library that enables agile analysis of I/O performance data. PyDarshan caters to both novice and advanced users by offering ready-to-use HTML reports as well as a rich collection of APIs to facilitate custom analyses. We present the design of PyDarshan and demonstrate its effectiveness in four diverse real-world analysis use cases.},\n  isbn = {979-8-4007-0785-8},\n  keywords = {High-Performance Computing,Input/Output,Performance Analysis,Storage}\n}\n"},79728:(e,n,t)=>{e.exports=t.p+"img/LLNL-long.988354ca.svg"},29748:(e,n,t)=>{e.exports=t.p+"img/darshan.42e56f3e.png"},96444:(e,n,t)=>{e.exports=t.p+"img/ethics-in-hpc.73370254.png"},56157:(e,n,t)=>{e.exports=t.p+"img/flux.0eb5b79e.png"},75520:(e,n,t)=>{e.exports=t.p+"img/hatchet.aaa603e6.png"},31545:(e,n,t)=>{e.exports=t.p+"img/iobat.5edaede1.svg"},92405:(e,n,t)=>{e.exports=t.p+"img/kubernetes.533af190.png"},14847:(e,n,t)=>{e.exports=t.p+"img/nsdf.dda46e1f.png"},325:(e,n,t)=>{e.exports=t.p+"img/thicket-logo.2e681a27.png"}}]);