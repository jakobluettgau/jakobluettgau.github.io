{"metadata":{"current_page":3,"total_pages":4},"posts":[{"title":"Demonstrating the Effect of Network Topology on Point-to-Point Latency in High-Performance Computing","authors":["Jakob Luettgau"],"datetime":"2023-03-19T12:41:00.000Z","text":"High-Performance Computing (HPC) applications are often tightly coupled and sensitive to latency between different processing elements but also to I/O contention due communication that flow through the same physical links. In most of todays supercomputers, the networks are shared resource which can lead to unpredictable runtime performance due to contention with other applications. The observed network performance in many cases is intimitely related to the underlying network topology. In many HPC systems, this topology is a fat tree which can lead to very characteristic latency patterns especially for large applications that span many nodes.","image":"./point-to-point.png","caption":null,"published":true,"id":"Demonstrating-the-Effect-of-Network-Topologies-in-HPC","url":"https://jakobluettgau.github.io/#/blog/posts/Demonstrating-the-Effect-of-Network-Topologies-in-HPC","date":"2023-03-19T12:41:00.000Z"},{"title":"Crafting a Quartet of Barstools","authors":["Jakob Luettgau"],"datetime":"2022-12-16T12:40:00.000Z","text":"This article documents the production of four barstools from design to final product. The project involved various forms of metal processing, from bending, to welding and treatment as well as woodworking for the seating elements and back support. The goal was to build a height-adjustable chair that would come out under a $100 per chair. ","image":"./thumb.jpg","caption":null,"published":true,"id":"Crafting-a-Quartett-of-Barstools","url":"https://jakobluettgau.github.io/#/blog/posts/Crafting-a-Quartett-of-Barstools","date":"2022-12-16T12:40:00.000Z"},{"title":"Machine Learning Fundamentals for Systems Optimization","authors":["Jakob Luettgau"],"datetime":"2022-03-30T17:40:31Z","text":"This post takes a closer look at the machanics of machine learning training and takes a lense what this means for training on performance data. Here we take a particular look at I/O performance data tied to specirfic scientific workflows, but many of the considerations should be true for other distributed systems. We look how, while the model is at the heart of learned approaches, a lot of the mechanics and important choices lie in the data generating distribution, the loss and regularization methods used to update the model across multiple iterations of training epochs.","image":"./svg/ml.png","caption":null,"published":true,"id":"Machine-Learning-Fundamentals-for-Systems-Optimization","url":"https://jakobluettgau.github.io/#/blog/posts/Machine-Learning-Fundamentals-for-Systems-Optimization","date":"2022-03-30T17:40:31.000Z"}]}