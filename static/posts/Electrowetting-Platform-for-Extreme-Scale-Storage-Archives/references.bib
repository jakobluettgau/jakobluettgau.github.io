@inproceedings{luettgauReproducingExtendingAnalytical,
  title = {Reproducing and {{Extending Analytical Performance Models}} of {{Generalized Hierarchical Scheduling}}},
  booktitle = {2nd {{Workshop}} on {{Reproducible Workflows}}, {{Data Management}}, and {{Security}}},
  author = {Luettgau, Jakob and {Caino-Lores}, Silvina and Suarez, Kae and Taufer, Michela},
  pages = {6},
  address = {{Salt Lake City, UT, USA}},
  abstract = {Workflows in High-Performance Computing (HPC) are rapidly changing towards more complex and large-scale workflows. In particular, high-throughput and ensemble workflows are becoming increasingly common. These workflows impose significant burden on current HPC scheduling systems which typically use slow, centralized schedulers. Generalized hierarchical scheduling (GHS) is a potential solution to face modern workflows but is not widely adopted in HPC yet. One difficulty hindering widespread adoption is the lack of performance models to configure and fit application requirements. The few existing models are often built on stick assumptions that can substantially reduce the analysis realism.},
  langid = {english}
}
% == BibTeX quality report for luettgauReproducingExtendingAnalytical:
% Missing required field 'year'
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Zotero")

@inproceedings{luttgauUnderstandingBehaviorHPC2018,
  title = {Toward {{Understanding I}}/{{O Behavior}} in {{HPC Workflows}}},
  booktitle = {2018 {{IEEE}}/{{ACM}} 3rd {{International Workshop}} on {{Parallel Data Storage}} \& {{Data Intensive Scalable Computing Systems}} ({{PDSW-DISCS}})},
  author = {L{\"u}ttgau, Jakob and Snyder, Shane and Carns, Philip and Wozniak, Justin M. and Kunkel, Julian and Ludwig, Thomas},
  year = {2018},
  month = nov,
  pages = {64--75},
  address = {{Dallas, TX, USA}},
  doi = {10.1109/PDSW-DISCS.2018.00012},
  abstract = {Scientific discovery increasingly depends on complex workflows consisting of multiple phases and sometimes millions of parallelizable tasks or pipelines. These workflows access storage resources for a variety of purposes, including preprocessing, simulation output, and postprocessing steps. Unfortunately, most workflow models focus on the scheduling and allocation of computational resources for tasks while the impact on storage systems remains a secondary objective and an open research question. I/O performance is not usually accounted for in workflow telemetry reported to users. In this paper, we present an approach to augment the I/O efficiency of the individual tasks of workflows by combining workflow description frameworks with system I/O telemetry data. A conceptual architecture and a prototype implementation for HPC data center deployments are introduced. We also identify and discuss challenges that will need to be addressed by workflow management and monitoring systems for HPC in the future. We demonstrate how real-world applications and workflows could benefit from the approach, and we show how the approach helps communicate performance-tuning guidance to users.},
  keywords = {Data models,Engines,HPC,I/O,Instrumentation,Monitoring,Pipelines,Storage,Task analysis,Telemetry,Tools,Visualization,Workflow}
}
% == BibTeX quality report for luttgauUnderstandingBehaviorHPC2018:
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("IEEE Xplore")
