(globalThis["webpackChunkquasar_tufte"]=globalThis["webpackChunkquasar_tufte"]||[]).push([[490],{59490:(e,t,a)=>{"use strict";a.r(t),a.d(t,{default:()=>ye});var n=a(59835),i=a(86970),o=a(68001),r=a.n(o),s=a(24416),l=a.n(s);const c=e=>((0,n.dD)("data-v-034726c0"),e=e(),(0,n.Cn)(),e),h={class:"tufte-body"},d=(0,n.Uk)(),u=(0,n.Uk)(),m=(0,n.Uk)(),p=c((()=>(0,n._)("h2",null,"Overview",-1))),g=(0,n.Uk)(),f=(0,n.Uk)("\nLet's start from an overview that puts the most important pieces to work.\n"),w=(0,n.Uk)(" illustrates the relations of different concepts and terms commonly used for machine learning algorithms.\n"),k=(0,n.Uk)(),b=c((()=>(0,n._)("img",{src:r(),style:{width:"650px"}},null,-1))),y=(0,n.Uk)(),U=(0,n.Uk)("\nBasic machine learning mechanics in a nutshell visualizing the relationship between some of the most important concepts in machine learning.\nThe colors highlight three areas that require specific research aligning machine learning research with applications in system optimization. Blue highlights areas related to the data-generation Process, red is concerned with model architecture which holds a lot of complexity but is kept abstract here, and yellow approaches where new regularization strategies may help-\nThis figure was originally created for a similar discussion in my doctoral thesis"),v=(0,n.Uk)(".\n      "),_=(0,n.Uk)(),x=(0,n.Uk)('\nThe lense we take here however is focussed less on the machine learning part but on it\'s application for system optimization when considering the mechanics of machine learning.\nNotation is aligned with "Deep Learning" by Ian Goodfellows et al. '),W=(0,n.Uk)(" for reference.\nWe use "),z=(0,n.Uk)("\\theta"),T=(0,n.Uk)(" to refer to the true model which can only be approximated.\nTo to so we sample both inputs "),q=(0,n.Uk)("x"),D=(0,n.Uk)(" and targets "),L=(0,n.Uk)("y"),B=(0,n.Uk)(" from a data generating distribution "),I=(0,n.Uk)("(x, y) \\sim \\hat{p}_{data}"),A=(0,n.Uk)(".\n"),G=(0,n.Uk)(),C=(0,n.Uk)("\nWe then define our model "),P=(0,n.Uk)("\\theta_m"),R=(0,n.Uk)(" which generates a prediction "),S=(0,n.Uk)("\\hat{y}"),H=(0,n.Uk)(".\nBy comparing the target "),N=(0,n.Uk)("y"),M=(0,n.Uk)(" with out prediction "),Z=(0,n.Uk)("\\hat{y}"),E=(0,n.Uk)(" we can calculate a Loss "),O=(0,n.Uk)("L"),Y=(0,n.Uk)(".\nThere is many possible definitions and \n"),j=(0,n.Uk)("L"),J=(0,n.Uk)(),F=c((()=>(0,n._)("img",{src:l(),style:{width:"300px"}},null,-1))),K=(0,n.Uk)(),Q=(0,n.Uk)("Relationship of bias and variance where to much bias when applied generally may result in being systematically of target and high variance affecting the spread of predictions."),V=(0,n.Uk)("\n\nis often a exchangeable function which can be used to impose regulating effects on the learning process.\nThis concept of regularization often allows learning algorithms to converge towards an acceptable model more rapidly, but is does so at a cost that reduces variance at the cost of increasing bias as illustrated in "),X=(0,n.Uk)(".\n\n\n\n  "),$=(0,n.Uk)(),ee=c((()=>(0,n._)("section",null,[(0,n._)("h2",null,"Data Generation Process"),(0,n.Uk)(),(0,n._)("p",null,"\nThe data-generation process (blue), in the context of scientific workflows\nand storage systems, for the most part, is concerned with telemetry \ncapture which needs to be combined with workflow and system topology\ninformation. Reconsidering the overview of common machine learning tasks, there\nis also potential to learn some of these combined intermediate \nrepresentations. In addition, to develop generalizable solutions applicable to\nmultiple systems, research into the data sampling process is needed, \nbecause many machine learning algorithms respond unfavorably to highly\ncorrelated features or samples that are not drawn randomly.\n")],-1))),te=(0,n.Uk)(),ae=c((()=>(0,n._)("h2",null,"Model Architectures",-1))),ne=(0,n.Uk)(),ie=(0,n.Uk)("\nModel architectures (red) require exploration because of constrained \nperformance envelopes when used for real-time decision-making on the one\nhand, but also because of the opportunity to exploit knowledge about\nstructural relationships that exist in instrumentation data, in workflows\nas well as in system topologies. Convolutional Neural Networks (CNNs) "),oe=(0,n.Uk)(", for example, work so well for\nimage data because they take advantage of the fact that there actually\nexists a causal meaning in localized features, where max-pooling allows\nsummarizing over large regions without requiring a lot of resources. \nSimilar optimization opportunities might also exist for storage and workflow\ndata. Two additional considerations that might occasionally conflict are\nfurther architectures that are easy to prune into less costly models in\ncontrast to architectures which allow for easy introspection.\n"),re=(0,n.Uk)(),se=c((()=>(0,n._)("section",null,[(0,n._)("h2",null,"Regularization"),(0,n.Uk)(),(0,n._)("p",null,"\nRegularization methods (yellow) and learning strategies, too, can be expected\n to benefit from domain adaptation. In particular, introducing factors such \nas throughput, data locality, and energy consumption into the\nloss function might yield learning algorithms which are more suitable to\nworkflow and storage optimization. At the same time, it can be attractive\nto avoid departing too far from the more general approaches and instead\nopt for preconditioning of data and reformulating problem statements\nreducing the effort of incorporating future research in this area. There is\nalso a hardware acceleration argument to this, as it would be less likely\nto find more exotic primitives implemented in hardware, although this is\nmost likely only relevant during training and not during inference.\n"),(0,n.Uk)()],-1))),le=(0,n.Uk)(),ce={key:0},he=c((()=>(0,n._)("h3",{class:"no-count"},"References",-1))),de=(0,n.Uk)(),ue={class:"fullwidth"};function me(e,t,a,o,r,s){const l=(0,n.up)("blog-metadata"),c=(0,n.up)("t-ref"),me=(0,n.up)("center"),pe=(0,n.up)("t-cite"),ge=(0,n.up)("t-figure"),fe=(0,n.up)("t-equation"),we=(0,n.up)("BibtexBibliography");return(0,n.wg)(),(0,n.iD)("div",h,[(0,n._)("article",null,[(0,n._)("section",null,[(0,n._)("h1",null,(0,i.zw)(e.metadata.title),1),d,(0,n.Wm)(l,{metadata:e.metadata},null,8,["metadata"]),u,(0,n._)("p",null,(0,i.zw)(e.metadata.text),1)]),m,(0,n._)("section",null,[p,g,(0,n._)("p",null,[f,(0,n.Wm)(c,{label:"fig:overview"}),w]),k,(0,n.Wm)(ge,{label:"fig:overview"},{caption:(0,n.w5)((()=>[U,(0,n.Wm)(pe,{name:"luttgauDecisionSupportWorkflowAware2021"}),v])),default:(0,n.w5)((()=>[(0,n.Wm)(me,null,{default:(0,n.w5)((()=>[b])),_:1}),y])),_:1}),_,(0,n._)("p",null,[x,(0,n.Wm)(pe,{name:"Goodfellow-et-al-2016"}),W,(0,n.Wm)(fe,{inline:""},{default:(0,n.w5)((()=>[z])),_:1}),T,(0,n.Wm)(fe,{inline:""},{default:(0,n.w5)((()=>[q])),_:1}),D,(0,n.Wm)(fe,{inline:""},{default:(0,n.w5)((()=>[L])),_:1}),B,(0,n.Wm)(fe,{inline:""},{default:(0,n.w5)((()=>[I])),_:1}),A]),G,(0,n._)("p",null,[C,(0,n.Wm)(fe,{inline:""},{default:(0,n.w5)((()=>[P])),_:1}),R,(0,n.Wm)(fe,{inline:""},{default:(0,n.w5)((()=>[S])),_:1}),H,(0,n.Wm)(fe,{inline:""},{default:(0,n.w5)((()=>[N])),_:1}),M,(0,n.Wm)(fe,{inline:""},{default:(0,n.w5)((()=>[Z])),_:1}),E,(0,n.Wm)(fe,{inline:""},{default:(0,n.w5)((()=>[O])),_:1}),Y,(0,n.Wm)(fe,{inline:""},{default:(0,n.w5)((()=>[j])),_:1}),J,(0,n.Wm)(ge,{side:"",label:"fig:bias-variance"},{caption:(0,n.w5)((()=>[Q])),default:(0,n.w5)((()=>[(0,n.Wm)(me,null,{default:(0,n.w5)((()=>[F])),_:1}),K])),_:1}),V,(0,n.Wm)(c,{label:"fig:bias-variance"}),X])]),$,ee,te,(0,n._)("section",null,[ae,ne,(0,n._)("p",null,[ie,(0,n.Wm)(pe,{name:"10.1109/5.726791"}),oe])]),re,se,le,e.references?((0,n.wg)(),(0,n.iD)("section",ce,[he,de,(0,n._)("p",ue,[(0,n.Wm)(we,{bibtex:e.references},null,8,["bibtex"])])])):(0,n.kq)("",!0)])])}var pe=a(60499),ge={title:"Machine Learning Fundamentals for Systems Optimization",authors:["Jakob Luettgau"],datetime:"2022-03-30T17:40:31Z",text:"This post takes a closer look at the machanics of machine learning training and takes a lense what this means for training on performance data. Here we take a particular look at I/O performance data tied to specirfic scientific workflows, but many of the considerations should be true for other distributed systems. We look how, while the model is at the heart of learned approaches, a lot of the mechanics and important choices lie in the data generating distribution, the loss and regularization methods used to update the model across multiple iterations of training epochs.",image:"./svg/ml.png",caption:null,published:!0},fe=a(79730).Z;const we=(0,n.aZ)({name:"BlogPost",components:{},setup(){const e=(0,pe.iH)(fe);return{metadata:ge,references:e}}});var ke=a(11639);const be=(0,ke.Z)(we,[["render",me],["__scopeId","data-v-034726c0"]]),ye=be},79730:(e,t,a)=>{"use strict";a.d(t,{Z:()=>n});const n='@article{10.1109/5.726791,\n  title = {Gradient-Based Learning Applied to Document Recognition},\n  author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},\n  year = {1998},\n  month = nov,\n  journal = {Proceedings of the IEEE},\n  volume = {86},\n  number = {11},\n  pages = {2278--2324},\n  issn = {1558-2256},\n  doi = {10.1109/5.726791},\n  abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.}\n}\n\n@book{Goodfellow-et-al-2016,\n  title = {Deep Learning},\n  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},\n  year = {2016},\n  publisher = {MIT Press}\n}\n\n@phdthesis{luttgauDecisionSupportWorkflowAware2021,\n  type = {Dissertation},\n  title = {Decision {{Support}} for {{Workflow-Aware High-Performance Storage Systems}}},\n  author = {L{\\"u}ttgau, Jakob},\n  year = {2021},\n  month = may,\n  address = {Hamburg},\n  url = {https://jakobluettgau.com/static/theses/luettgau_decision-support-for-workflow-aware-high-performance-storage-systems.pdf},\n  langid = {english},\n  school = {Universit{\\"a}t Hamburg}\n}\n'},24416:(e,t,a)=>{e.exports=a.p+"img/bias-variance.0ceb278b.png"},68001:(e,t,a)=>{e.exports=a.p+"img/ml.0a386299.png"}}]);