(globalThis["webpackChunkquasar_tufte"]=globalThis["webpackChunkquasar_tufte"]||[]).push([[750],{47750:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>re});var o=n(59835),a=n(86970),i=n(92350),s=n.n(i),r=n(88270),l=n.n(r),c=n(15813),h=n.n(c);const p=e=>((0,o.dD)("data-v-50dd36f9"),e=e(),(0,o.Cn)(),e),u={class:"tufte-body"},d=(0,o.Uk)(),m=(0,o.Uk)(),g=(0,o.Uk)(),f=p((()=>(0,o._)("h2",null,"The Benchmark",-1))),y=(0,o.Uk)(),b=p((()=>(0,o._)("img",{src:s(),style:{"max-width":"300px"}},null,-1))),w=(0,o.Uk)(),k=(0,o.Uk)("Sequence diagram of the benchmark and how times are measured."),v=(0,o.Uk)("\n\nTo demonstrate this behavior we use a custom micro-benchmark that can schedule \ndifferent point-to-point measurements.\nIt uses a combination of MPI for Python\n"),P=(0,o.Uk)(),U=(0,o.Uk)("\nto obtain information about the allocation (all the workers) and ZeroMQ \n"),M=(0,o.Uk)(),_=(0,o.Uk)("\nto orchestrate transfers.\nThe structure of the benchmark including how timings are taken is illustrated in \n"),I=(0,o.Uk)(".\nThe coordinator instructs different hosts to time messages from source to target in the most simple scenario.\nFor each transfer the source logs message sizes, number of operations, and the time it takes for a message to be send and acknowledged by the target.\n"),T=(0,o.Uk)(),x=p((()=>(0,o._)("h2",null,"Point-to-Point Measurement Results",-1))),W=(0,o.Uk)(),C=(0,o.Uk)("\n  The benchmark is then used to obtain a couple of measurements between each of the hosts for a small transfer of only a few bytes to get an idea of latency between hosts.\n  All the results are then visualized in "),Z=(0,o.Uk)(" as point-to-point communicaiton matrix. \n  This view nicely shows how there is groups of hosts that achieve high performance between each other but low performance with other groups.\n"),S=(0,o.Uk)(),z=p((()=>(0,o._)("img",{src:l()},null,-1))),j=(0,o.Uk)(),D=(0,o.Uk)("Operations per seconds for different node pairs. The sender is on the x axis and the receiver is on the y axis. The color is based on the operations per second with green indicating high performance and redder tones indicating low percormance."),Q=(0,o.Uk)(),q=p((()=>(0,o._)("h2",null,"Topology Overlays: Making Sense of the Results",-1))),O=(0,o.Uk)(),B=(0,o.Uk)("\n  While the point-to-point communication matrix illustrates the effect across the system quite well,\n  it does not offer any intuition to why the clusters of similar point-to-point performance exist in the first place.\n  It becomes very obvious, however, when overlaying the performance information over the topology.\n  This topology over is shown in "),L=(0,o.Uk)(" and is generated using Graphviz "),E=(0,o.Uk)(".\n  The "),G=p((()=>(0,o._)("span",{class:"emph"},"src",-1))),H=(0,o.Uk)(" is highlighted with a bold black border, all other nodes are at the receiving end of the measurment. It becomes very obvious that all nodes that are connected to the same switch share low latencies, and as we move away more hops the latency increases.\nThe graph corresponds to a single column taken out of "),A=(0,o.Uk)(".\n"),R=(0,o.Uk)(),F=p((()=>(0,o._)("img",{src:h()},null,-1))),J=(0,o.Uk)(),N=(0,o.Uk)("Operations per seconds from a single source (bold black border) to other nodes overlayed onto the network topology. Note how other hosts only one hop away are fast (green) and other more distant hosts requiring multiple hops add latency and thus decrease the total operations per second."),Y=(0,o.Uk)(),X={key:0},K=p((()=>(0,o._)("h3",{class:"no-count"},"References",-1))),V=(0,o.Uk)(),$={class:"fullwidth"};function ee(e,t,n,i,s,r){const l=(0,o.up)("blog-metadata"),c=(0,o.up)("t-figure"),h=(0,o.up)("t-cite"),p=(0,o.up)("t-ref"),ee=(0,o.up)("BibtexBibliography");return(0,o.wg)(),(0,o.iD)("div",u,[(0,o._)("article",null,[(0,o._)("section",null,[(0,o._)("h1",null,(0,a.zw)(e.metadata.title),1),d,(0,o.Wm)(l,{metadata:e.metadata},null,8,["metadata"]),m,(0,o._)("p",null,(0,a.zw)(e.metadata.text),1)]),g,(0,o._)("section",null,[f,y,(0,o._)("p",null,[(0,o.Wm)(c,{side:"",label:"fig:benchmark"},{caption:(0,o.w5)((()=>[k])),default:(0,o.w5)((()=>[b,w])),_:1}),v,(0,o.Wm)(h,{name:"10.1109/MCSE.2021.3083216"}),P,(0,o.Wm)(h,{name:"10.1016/j.jpdc.2005.03.010"}),U,(0,o.Wm)(h,{name:"ZeroMQ"}),M,(0,o.Wm)(h,{name:"PyZMQPythonbindings2023"}),_,(0,o.Wm)(p,{label:"fig:benchmark"}),I])]),T,(0,o._)("section",null,[x,W,(0,o._)("p",null,[C,(0,o.Wm)(p,{label:"fig:point-to-point"}),Z]),S,(0,o.Wm)(c,{label:"fig:point-to-point"},{caption:(0,o.w5)((()=>[D])),default:(0,o.w5)((()=>[z,j])),_:1})]),Q,(0,o._)("section",null,[q,O,(0,o._)("p",null,[B,(0,o.Wm)(p,{label:"fig:topology-overlay"}),L,(0,o.Wm)(h,{name:"GraphvizOpenSource"}),E,G,H,(0,o.Wm)(p,{label:"fig:point-to-point"}),A]),R,(0,o.Wm)(c,{label:"fig:topology-overlay"},{caption:(0,o.w5)((()=>[N])),default:(0,o.w5)((()=>[F,J])),_:1})]),Y,e.references?((0,o.wg)(),(0,o.iD)("section",X,[K,V,(0,o._)("p",$,[(0,o.Wm)(ee,{bibtex:e.references},null,8,["bibtex"])])])):(0,o.kq)("",!0)])])}var te=n(60499),ne={title:"Demonstrating the Effect of Network Topology on Point-to-Point Latency in High-Performance Computing",authors:["Jakob Luettgau"],datetime:"2023-03-19T12:41:00.000Z",text:"High-Performance Computing (HPC) applications are often tightly coupled and sensitive to latency between different processing elements but also to I/O contention due communication that flow through the same physical links. In most of todays supercomputers, the networks are shared resource which can lead to unpredictable runtime performance due to contention with other applications. The observed network performance in many cases is intimitely related to the underlying network topology. In many HPC systems, this topology is a fat tree which can lead to very characteristic latency patterns especially for large applications that span many nodes.",image:"./point-to-point.png",caption:null,published:!0},oe=n(88376).Z;const ae=(0,o.aZ)({name:"BlogPost",components:{},setup(){const e=(0,te.iH)(oe);return{metadata:ne,references:e}}});var ie=n(11639);const se=(0,ie.Z)(ae,[["render",ee],["__scopeId","data-v-50dd36f9"]]),re=se},88376:(e,t,n)=>{"use strict";n.d(t,{Z:()=>o});const o="@article{10.1016/j.jpdc.2005.03.010,\n  title = {{{MPI}} for {{Python}}},\n  author = {Dalc{\\'i}n, Lisandro and Paz, Rodrigo and Storti, Mario},\n  year = {2005},\n  month = sep,\n  journal = {Journal of Parallel and Distributed Computing},\n  volume = {65},\n  number = {9},\n  pages = {1108--1115},\n  issn = {0743-7315},\n  doi = {10.1016/j.jpdc.2005.03.010},\n  url = {https://www.sciencedirect.com/science/article/pii/S0743731505000560},\n  urldate = {2023-08-13},\n  abstract = {MPI for Python provides bindings of the Message Passing Interface (MPI) standard for the Python programming language and allows any Python program to exploit multiple processors. This package is constructed on top of the MPI-1 specification and defines an object-oriented interface which closely follows MPI-2 C++bindings. It supports point-to-point (sends, receives) and collective (broadcasts, scatters, gathers) communications of general Python objects. Efficiency has been tested in a Beowulf class cluster and satisfying results were obtained. MPI for Python is open source and available for download on the web (http://www.cimec.org.ar/python).},\n  langid = {english}\n}\n\n@article{10.1109/MCSE.2021.3083216,\n  title = {Mpi4py: {{Status Update After}} 12 {{Years}} of {{Development}}},\n  shorttitle = {Mpi4py},\n  author = {Dalcin, Lisandro and Fang, Yao-Lung L.},\n  year = {2021},\n  month = jul,\n  journal = {Computing in Science \\& Engineering},\n  volume = {23},\n  number = {4},\n  pages = {47--54},\n  issn = {1558-366X},\n  doi = {10.1109/MCSE.2021.3083216},\n  abstract = {MPI for Python (mpi4py) has evolved to become the most used Python binding for the message passing interface (MPI). We report on various improvements and features that mpi4py gradually accumulated over the past decade, including support up to the MPI-3.1 specification, support for CUDA-aware MPI implementations, and other utilities at the intersection of MPI-based parallel distributed computing and Python application development.}\n}\n\n@misc{GraphvizOpenSource,\n  title = {Graphviz--- {{Open Source Graph Drawing Tools}} {\\textbar} {{SpringerLink}}},\n  url = {https://link.springer.com/chapter/10.1007/3-540-45848-4_57},\n  urldate = {2023-08-13}\n}\n\n@misc{PyZMQPythonbindings2023,\n  title = {{{PyZMQ}}: {{Python}} Bindings for {{{\\O}MQ}}},\n  shorttitle = {{{PyZMQ}}},\n  year = {2023},\n  month = aug,\n  url = {https://github.com/zeromq/pyzmq},\n  urldate = {2023-08-13},\n  abstract = {PyZMQ:  Python bindings for zeromq},\n  copyright = {BSD-3-Clause},\n  howpublished = {The ZeroMQ project}\n}\n\n@misc{ZeroMQ,\n  title = {{{ZeroMQ}}},\n  url = {https://zeromq.org/},\n  urldate = {2023-08-13},\n  abstract = {An open-source universal messaging library}\n}\n"},92350:(e,t,n)=>{e.exports=n.p+"img/benchmark.ca43db74.png"},88270:(e,t,n)=>{e.exports=n.p+"img/point-to-point.890e4682.png"},15813:(e,t,n)=>{e.exports=n.p+"img/topology-overlay.c048d08c.png"}}]);