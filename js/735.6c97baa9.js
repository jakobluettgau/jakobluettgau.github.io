"use strict";(globalThis["webpackChunkquasar_tufte"]=globalThis["webpackChunkquasar_tufte"]||[]).push([[735],{80735:(e,n,a)=>{a.r(n),a.d(n,{default:()=>B});var t=a(59835);const i={class:"tufte-body"},o=(0,t._)("h1",null,"Publications",-1),r=(0,t.Uk)(),s=(0,t._)("h2",{id:"/publications/#pubs-journals",class:"no-count"},"Journal Articles",-1),l=(0,t.Uk)(),d={class:"fullwidth"},c=(0,t.Uk)(),u=(0,t._)("h2",{id:"/publications/#pubs-conferences",class:"no-count"},"Conference Papers",-1),h=(0,t.Uk)(),g={class:"fullwidth"},m=(0,t.Uk)(),p=(0,t._)("h2",{id:"/publications/#pubs-workshops",class:"no-count"},"Workshop Papers",-1),f=(0,t.Uk)(),b={class:"fullwidth"},y=(0,t.Uk)(),w=(0,t._)("h2",{id:"/publications/#pubs-posters",class:"no-count"},"Posters",-1),v=(0,t.Uk)(),k={class:"fullwidth"},S=(0,t.Uk)(),C=(0,t._)("h2",{id:"/publications/#pubs-reports",class:"no-count"},"Reports",-1),P=(0,t.Uk)(),I={class:"fullwidth"},D=(0,t.Uk)(),T=(0,t._)("h2",{id:"/publications/#pubs-books",class:"no-count"},"Book Chapters",-1),E=(0,t.Uk)(),A={class:"fullwidth"},L=(0,t.Uk)(),H=(0,t._)("h2",{id:"/publications/#pubs-theses",class:"no-count"},"Theses",-1),x=(0,t.Uk)(),M={class:"fullwidth"};function J(e,n,a,J,z,O){const F=(0,t.up)("BibtexBibliography");return(0,t.wg)(),(0,t.iD)("div",i,[(0,t._)("article",null,[(0,t._)("section",null,[o,r,s,l,(0,t._)("p",d,[(0,t.Wm)(F,{bibtex:e.journals,mode:"all",highlightCallback:e.highlighter},null,8,["bibtex","highlightCallback"])])]),c,(0,t._)("section",null,[u,h,(0,t._)("p",g,[(0,t.Wm)(F,{bibtex:e.conferences,mode:"all",highlightCallback:e.highlighter},null,8,["bibtex","highlightCallback"])])]),m,(0,t._)("section",null,[p,f,(0,t._)("p",b,[(0,t.Wm)(F,{bibtex:e.workshops,mode:"all",highlightCallback:e.highlighter},null,8,["bibtex","highlightCallback"])])]),y,(0,t._)("section",null,[w,v,(0,t._)("p",k,[(0,t.Wm)(F,{bibtex:e.posters,mode:"all",highlightCallback:e.highlighter},null,8,["bibtex","highlightCallback"])])]),S,(0,t._)("section",null,[C,P,(0,t._)("p",I,[(0,t.Wm)(F,{bibtex:e.reports,mode:"all",highlightCallback:e.highlighter},null,8,["bibtex","highlightCallback"])])]),D,(0,t._)("section",null,[T,E,(0,t._)("p",A,[(0,t.Wm)(F,{bibtex:e.book_chapters,mode:"all",highlightCallback:e.highlighter},null,8,["bibtex","highlightCallback"])])]),L,(0,t._)("section",null,[H,x,(0,t._)("p",M,[(0,t.Wm)(F,{bibtex:e.theses,mode:"all",highlightCallback:e.highlighter},null,8,["bibtex","highlightCallback"])])])])])}var z=a(60499),O=a(98332).Z,F=a(90234).Z,W=a(50400).Z,N=a(79741).Z,_=a(72984).Z;const U=(0,t.aZ)({name:"IndexPage",components:{},setup(){const e=(0,z.iH)(O),n=(0,z.iH)(a(80033).Z),t=(0,z.iH)(a(98904).Z),i=(0,z.iH)(F),o=(0,z.iH)(W),r=(0,z.iH)(N),s=(0,z.iH)(_);function l(e){return e=e.replace(/Luettgau, J./g,"<b>Luettgau, J.</b>"),e=e.replace(/LÃ¼ttgau, J./g,"<b>Luettgau, J.</b>"),e}return{journals:e,conferences:n,reports:i,posters:o,book_chapters:r,workshops:t,theses:s,highlighter:l}}});var j=a(11639);const K=(0,j.Z)(U,[["render",J]]),B=K},79741:(e,n,a)=>{a.d(n,{Z:()=>t});const t='@incollection{ICIKL17,\n  title = {Interaktiver {{C-Programmierkurs}}, {{ICP}}},\n  booktitle = {{{HOOU}} Content Projekte Der Vorprojektphase 2015/16 -- Sonderband Zum Fachmagazin Synergie},\n  author = {Kunkel, Julian and L{\\"u}ttgau, Jakob},\n  year = {2017},\n  month = apr,\n  pages = {182--186},\n  publisher = {Universit{\\"a}t Hamburg},\n  address = {Universit{\\"a}t Hamburg, Mittelweg 177, 20148 Hamburg},\n  url = {https://www.synergie.uni-hamburg.de/media/sonderbaende/hoou-content-projekte-2015-2016.pdf},\n  abstract = {Programmiersprachen bilden die Basis f{\\"u}r die automatisierte Datenverarbeitung in der digitalen Welt. Obwohl die Grundkonzepte einfach zu verstehen sind, beherrscht nur ein geringer Anteil von Personen diese Werkzeuge. Die Gr{\\"u}nde hierf{\\"u}r sind Defizite in der Ausbildung und die hohe Einstiegsh{\\"u}rde bei der Bereitstellung einer produktiven Programmierumgebung. Insbesondere erfordert das Erlernen einer Programmiersprache die praktische Anwendung der Sprache, vergleichbar mit dem Erlernen einer Fremdsprache. Ziel des Projekts ist die Erstellung eines interaktiven Kurses f{\\"u}r die Lehre der Programmiersprache C. Die Interaktivit{\\"a}t und das angebotene automatische Feedback sind an den Bed{\\"u}rfnissen der Teilnehmerinnen und Teilnehmer orientiert und bieten die M{\\"o}glichkeit, autodidaktisch Kenntnisse auf- und auszubauen. Die Lektionen beinhalten sowohl die Einf{\\"u}hrung in spezifische Teilthemen als auch anspruchsvollere Aufgaben, welche die akademischen Probleml{\\"o}sef{\\"a}higkeiten f{\\"o}rdern. Damit werden unterschiedliche akademische Zielgruppen bedient und aus verschieden Bereichen der Zivilgesellschaft an die Informatik herangef{\\"u}hrt. Der in diesem Projekt entwickelte Programmierkurs und die Plattform zur Programmierung k{\\"o}nnen weltweit frei genutzt werden, und der Quellcode bzw. die Lektionen stehen unter Open-Source-Lizenzen und k{\\"o}nnen deshalb beliebig auf die individuellen Bed{\\"u}rfnisse angepasst werden. Dies erm{\\"o}glicht insbesondere das Mitmachen und Besteuern von neuen Lektionen zur Plattform.},\n  isbn = {978-3-924330-57-6}\n}\n'},98332:(e,n,a)=>{a.d(n,{Z:()=>t});const t='@article{10.1002/gdj3.132,\n  title = {To the Brave Scientists: {{Aren}}\'t We Strong Enough to Stand (and Profit from) Uncertainty in {{Earth}} System Measurement and Modelling?},\n  shorttitle = {To the Brave Scientists},\n  author = {Paasche, Hendrik and Gross, Matthias and L{\\"u}ttgau, Jakob and Greenberg, David S. and Weigel, Tobias},\n  year = {2021},\n  month = sep,\n  journal = {Geoscience Data Journal},\n  issn = {2049-6060},\n  doi = {10.1002/gdj3.132},\n  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/gdj3.132},\n  urldate = {2022-07-11},\n  abstract = {The current handling of data in earth observation, modelling and prediction measures gives cause for critical consideration, since we all too often carelessly ignore data uncertainty. We think that Earth scientists are generally aware of the importance of linking data to quantitative uncertainty measures. But we also think that uncertainty quantification of Earth observation data too often fails at very early stages. We claim that data acquisition without uncertainty quantification is not sustainable and machine learning and computational modelling cannot unfold their potential when analysing complex natural systems like the Earth. Current approaches such as stochastic perturbation of parameters or initial conditions cannot quantify uncertainty or bias arising from the choice of model, limiting scientific progress. We need incentives stimulating the honest treatment of uncertainty starting during data acquisition, continuing through analysis methodology and prediction results. Computational modellers and machine learning experts have a critical role, since they enjoy high esteem from stakeholders and their methodologies and their results critically depend on data uncertainty. If both want to advance their uncertainty assessment of models and predictions of complex systems like the Earth, they have a common problem to solve. Together, computational modellers and machine learners could develop new strategies for bias identification and uncertainty quantification offering a more all-embracing uncertainty quantification than any known methodology. But since it starts for computational modellers and machine learners with data and their uncertainty, the fundamental first step in such a development would be leveraging shareholder esteem to insistently advocate for reduction of ignorance when it comes to uncertainty quantification of data.},\n  langid = {english},\n  keywords = {computational modelling,data uncertainty,machine learning,model uncertainty,uncertainty quantification}\n}\n\n@article{10.1109/MCSE.2024.3489732,\n  title = {Converged {{Computing}}: {{A Best}} of {{Both Worlds}} of {{High-Performance Computing}} and {{Cloud}}},\n  author = {Sochat, Vanessa V. and Milroy, Daniel and Misale, Claudia and Luettgau, Jakob and Bollig, Evan F. and Magro, William},\n  year = {2024},\n  journal = {Computing in Science \\& Engineering},\n  volume = {26},\n  number = {3},\n  pages = {4--7},\n  doi = {10.1109/MCSE.2024.3489732},\n  url = {https://doi.org/10.1109/MCSE.2024.3489732},\n  keywords = {/unread}\n}\n\n@article{10.1145/3544497.3544508,\n  title = {Data-{{Aware Compression}} for {{HPC}} Using {{Machine Learning}}},\n  author = {Plehn, Julius and Fuchs, Anna and Kuhn, Michael and L{\\"u}ttgau, Jakob and Ludwig, Thomas},\n  year = {2022},\n  month = jun,\n  journal = {ACM SIGOPS Operating Systems Review},\n  volume = {56},\n  number = {; Issue 1},\n  pages = {62--69},\n  issn = {0163-5980},\n  doi = {10.1145/3544497.3544508},\n  url = {https://doi.org/10.1145/3544497.3544508},\n  urldate = {2022-07-30},\n  abstract = {While compression can provide significant storage and cost savings, its use within HPC applications is often only of secondary concern. This is in part due to the inflexibility of existing approaches where a single compression algorithm has to be used throughout the whole application but also because insights into the behaviour of the algorithms within the context of individual applications are missing.}\n}\n\n@article{10.1177/10943420231167800,\n  ids = {zhouOrchestrationMaterialsScience2023a},\n  title = {Orchestration of Materials Science Workflows for Heterogeneous Resources at Large Scale},\n  author = {Zhou, Naweiluo and Scorzelli, Giorgio and Luettgau, Jakob and Kancharla, Rahul R and Kane, Joshua J and Wheeler, Robert and Croom, Brendan P and Newell, Pania and Pascucci, Valerio and Taufer, Michela},\n  year = {2023},\n  month = jul,\n  journal = {The International Journal of High Performance Computing Applications},\n  volume = {37},\n  number = {3-4},\n  pages = {260--271},\n  publisher = {SAGE Publications Ltd STM},\n  issn = {1094-3420},\n  doi = {10.1177/10943420231167800},\n  url = {https://doi.org/10.1177/10943420231167800},\n  urldate = {2024-01-22},\n  abstract = {In the era of big data, materials science workflows need to handle large-scale data distribution, storage, and computation. Any of these areas can become a performance bottleneck. We present a framework for analyzing internal material structures (e.g., cracks) to mitigate these bottlenecks. We demonstrate the effectiveness of our framework for a workflow performing synchrotron X-ray computed tomography reconstruction and segmentation of a silica-based structure. Our framework provides a cloud-based, cutting-edge solution to challenges such as growing intermediate and output data and heavy resource demands during image reconstruction and segmentation. Specifically, our framework efficiently manages data storage, scaling up compute resources on the cloud. The multi-layer software structure of our framework includes three layers. A top layer uses Jupyter notebooks and serves as the user interface. A middle layer uses Ansible for resource deployment and managing the execution environment. A low layer is dedicated to resource management and provides resource management and job scheduling on heterogeneous nodes (i.e., GPU and CPU). At the core of this layer, Kubernetes supports resource management, and Dask enables large-scale job scheduling for heterogeneous resources. The broader impact of our work is four-fold: through our framework, we hide the complexity of the cloud\'s software stack to the user who otherwise is required to have expertise in cloud technologies; we manage job scheduling efficiently and in a scalable manner; we enable resource elasticity and workflow orchestration at a large scale; and we facilitate moving the study of nonporous structures, which has wide applications in engineering and scientific fields, to the cloud. While we demonstrate the capability of our framework for a specific materials science application, it can be adapted for other applications and domains because of its modular, multi-layer architecture.},\n  langid = {english}\n}\n\n@article{10.14529/jsfi180103,\n  title = {Survey of {{Storage Systems}} for {{High-Performance Computing}}},\n  author = {L{\\"u}ttgau, Jakob and Kuhn, Michael and Duwe, Kira and Alforov, Yevhen and Betke, Eugen and Kunkel, Julian and Ludwig, Thomas},\n  year = {2018},\n  month = apr,\n  journal = {Supercomputing Frontiers and Innovations},\n  volume = {5},\n  number = {1},\n  pages = {31--58},\n  issn = {2313-8734},\n  doi = {10.14529/jsfi180103},\n  url = {https://superfri.org/index.php/superfri/article/view/162},\n  urldate = {2022-07-30},\n  abstract = {In current supercomputers, storage is typically provided by parallel distributed file systems for hot data and tape archives for cold data. These file systems are often compatible with local file systems due to their use of the POSIX interface and semantics, which eases development and debugging because applications can easily run both on workstations and supercomputers. There is a wide variety of file systems to choose from, each tuned for different use cases and implementing different optimizations. However, the overall application performance is often held back by I/O bottlenecks due to insufficient performance of file systems or I/O libraries for highly parallel workloads. Performance problems are dealt with using novel storage hardware technologies as well as alternative I/O semantics and interfaces. These approaches have to be integrated into the storage stack seamlessly to make them convenient to use. Upcoming storage systems abandon the traditional POSIX interface and semantics in favor of alternative concepts such as object and key-value storage; moreover, they heavily rely on technologies such as NVM and burst buffers to improve performance. Additional tiers of storage hardware will increase the importance of hierarchical storage management. Many of these changes will be disruptive and require application developers to rethink their approaches to data management and I/O. A thorough understanding of today\'s storage infrastructures, including their strengths and weaknesses, is crucially important for designing and implementing scalable storage systems suitable for demands of exascale computing.},\n  copyright = {Copyright (c)},\n  langid = {english}\n}\n\n@article{10.14529/jsfi200101,\n  title = {State of the {{Art}} and {{Future Trends}} in {{Data Reduction}} for {{High-Performance Computing}}},\n  author = {Duwe, Kira and L{\\"u}ttgau, Jakob and Mania, Georgiana and Squar, Jannek and Fuchs, Anna and Kuhn, Michael and Betke, Eugen and Ludwig, Thomas},\n  year = {2020},\n  month = apr,\n  journal = {Supercomputing Frontiers and Innovations},\n  volume = {7},\n  number = {1},\n  pages = {4--36},\n  issn = {2313-8734},\n  doi = {10.14529/jsfi200101},\n  url = {https://superfri.org/index.php/superfri/article/view/303},\n  urldate = {2022-07-30},\n  abstract = {Research into data reduction techniques has gained popularity in recent years as storage capacity and performance become a growing concern. This survey paper provides an overview of leveraging points found in high-performance computing (HPC) systems and suitable mechanisms to reduce data volumes. We present the underlying theories and their application throughout the HPC stack and also discuss related hardware acceleration and reduction approaches. After introducing relevant use-cases, an overview of modern lossless and lossy compression algorithms and their respective usage at the application and file system layer is given. In anticipation of their increasing relevance for adaptive and in situ approaches, dimensionality reduction techniques are summarized with a focus on non-linear feature extraction. Adaptive approaches and in situ compression algorithms and frameworks follow. The key stages and new opportunities to deduplication are covered next. An unconventional but promising method is recomputation, which is proposed at last. We conclude the survey with an outlook on future developments.},\n  copyright = {Copyright (c)},\n  langid = {english}\n}\n\n@article{ICIKL16,\n  title = {Interaktiver {{C-Programmierkurs}}, {{ICP}}},\n  author = {Kunkel, Julian and L{\\"u}ttgau, Jakob},\n  year = {2016},\n  month = nov,\n  journal = {Synergie, Fachmagazin f{\\"u}r Digitalisierung in der Lehre},\n  number = {2},\n  pages = {74--75},\n  url = {https://uhh.de/cp3i1},\n  abstract = {Programmiersprachen bilden die Basis f{\\"u}r die automatisierte Datenverarbeitung in der digitalen Welt. Obwohl die Grundkonzepte einfach zu verstehen sind, beherrscht nur ein geringer Anteil von Personen diese Werkzeuge. Die Gr{\\"u}nde hierf{\\"u}r sind Defizite in der Ausbildung und die hohe Einstiegsh{\\"u}rde bei der Bereitstellung einer produktiven Programmierumgebung. Insbesondere erfordert das Erlernen einer Programmiersprache die praktische Anwendung der Sprache, vergleichbar mit dem Erlernen einer Fremdsprache. Ziel des Projekts ist die Erstellung eines interaktiven Kurses f{\\"u}r die Lehre der Programmiersprache C. Die Interaktivit{\\"a}t und das angebotene automatische Feedback sind an den Bed{\\"u}rfnissen der Teilnehmerinnen und Teilnehmer orientiert und bieten die M{\\"o}glichkeit, autodidaktisch Kenntnisse auf- und auszubauen. Die Lektionen beinhalten sowohl die Einf{\\"u}hrung in spezifische Teilthemen als auch anspruchsvollere Aufgaben, welche die akademischen Probleml{\\"o}sef{\\"a}higkeiten f{\\"o}rdern. Damit werden unterschiedliche akademische Zielgruppen bedient und aus verschieden Bereichen der Zivilgesellschaft an die Informatik herangef{\\"u}hrt. Der in diesem Projekt entwickelte Programmierkurs und die Plattform zur Programmierung k{\\"o}nnen weltweit frei genutzt werden, und der Quellcode bzw. die Lektionen stehen unter Open-Source-Lizenzen und k{\\"o}nnen deshalb beliebig auf die individuellen Bed{\\"u}rfnisse angepasst werden. Dies erm{\\"o}glicht insbesondere das Mitmachen und Besteuern von neuen Lektionen zur Plattform.}\n}\n'},80033:(e,n,a)=>{a.d(n,{Z:()=>t});const t="@inproceedings{10.1007/978-3-031-34668-2_25,\n  ids = {Jakob_hci23},\n  title = {Development of~{{Large-Scale Scientific Cyberinfrastructure}} and~the~{{Growing Opportunity}} to~{{Democratize Access}} to~{{Platforms}} and~{{Data}}},\n  booktitle = {Distributed, {{Ambient}} and {{Pervasive Interactions}}},\n  author = {Luettgau, Jakob and Scorzelli, Giorgio and Pascucci, Valerio and Taufer, Michela},\n  editor = {Streitz, Norbert A. and Konomi, Shin'ichi},\n  year = {2023},\n  series = {Lecture {{Notes}} in {{Computer Science}}},\n  pages = {378--389},\n  publisher = {Springer Nature Switzerland},\n  address = {Cham},\n  doi = {10.1007/978-3-031-34668-2_25},\n  url = {https://doi.org/10.1007/978-3-031-34668-2_25},\n  abstract = {As researchers across scientific domains rapidly adopt advanced scientific computing methodologies, access to advanced cyberinfrastructure (CI) becomes a critical requirement in scientific discovery. Lowering the entry barriers to CI is a crucial challenge in interdisciplinary sciences requiring frictionless software integration, data sharing from many distributed sites, and access to heterogeneous computing platforms. In this paper, we explore how the challenge is not merely a factor of availability and affordability of computing, network, and storage technologies but rather the result of insufficient interfaces with an increasingly heterogeneous mix of computing technologies and data sources. With more distributed computation and data, scientists, educators, and students must invest their time and effort in coordinating data access and movements, often penalizing their scientific research. Investments in the interfaces' software stack are necessary to help scientists, educators, and students across domains take advantage of advanced computational methods. To this end, we propose developing a science data fabric as the standard scientific discovery interface that seamlessly manages data dependencies within scientific workflows and CI.},\n  isbn = {978-3-031-34668-2},\n  langid = {english},\n  keywords = {Cyberinfrastructure,National Science Data Fabric,Scientific data}\n}\n\n@inproceedings{10.1007/978-3-319-07518-1_16,\n  title = {The {{SIOX Architecture}} -- {{Coupling Automatic Monitoring}} and {{Optimization}} of {{Parallel I}}/{{O}}},\n  booktitle = {Supercomputing},\n  author = {Kunkel, Julian M. and Zimmer, Michaela and H{\\\"u}bbe, Nathanael and Aguilera, Alvaro and Mickler, Holger and Wang, Xuan and Chut, Andriy and B{\\\"o}nisch, Thomas and L{\\\"u}ttgau, Jakob and Michel, Roman and Weging, Johann},\n  editor = {Kunkel, Julian Martin and Ludwig, Thomas and Meuer, Hans Werner},\n  year = {2014},\n  series = {Lecture {{Notes}} in {{Computer Science}}},\n  pages = {245--260},\n  publisher = {Springer International Publishing},\n  address = {Cham},\n  doi = {10.1007/978-3-319-07518-1_16},\n  url = {http://doi.org/10.1007/978-3-319-07518-1_16},\n  abstract = {Performance analysis and optimization of high-performance I/O systems is a daunting task. Mainly, this is due to the overwhelmingly complex interplay of the involved hardware and software layers. The Scalable I/O for Extreme Performance (SIOX) project provides a versatile environment for monitoring I/O activities and learning from this information. The goal of SIOX is to automatically suggest and apply performance optimizations, and to assist in locating and diagnosing performance problems.},\n  isbn = {978-3-319-07518-1},\n  langid = {english},\n  keywords = {Machine Learning,Parallel I/O,Performance Optimization}\n}\n\n@inproceedings{10.1109/CLOUD60044.2023.00052,\n  title = {Enabling {{Scalability}} in the {{Cloud}} for {{Scientific Workflows}}: {{An Earth Science Use Case}}},\n  shorttitle = {Enabling {{Scalability}} in the {{Cloud}} for {{Scientific Workflows}}},\n  booktitle = {2023 {{IEEE}} 16th {{International Conference}} on {{Cloud Computing}} ({{CLOUD}})},\n  author = {Olaya, Paula and Luettgau, Jakob and Roa, Camila and Llamas, Ricardo and Vargas, Rodrigo and Wen, Sophia and Chung, I-Hsin and Seelam, Seetharami and Park, Yoonho and Lofstead, Jay and Taufer, Michela},\n  year = {2023},\n  month = jul,\n  pages = {383--393},\n  issn = {2159-6190},\n  doi = {10.1109/CLOUD60044.2023.00052},\n  url = {https://doi.org/10.1109/CLOUD60044.2023.00052},\n  urldate = {2024-02-06},\n  abstract = {Scientific discovery increasingly relies on interoperable, multimodular workflows generating intermediate data. The complexity of managing intermediate data may cause performance losses or unexpected costs. This paper defines an approach to composing these scientific workflows on cloud services, focusing on workflow data orchestration, management, and scalability. We demonstrate the effectiveness of our approach with the SOMOSPIE scientific workflow that deploys machine learning (ML) models to predict high-resolution soil moisture using an HPC service (LSF) and an open-source cloud-native service (K8s) and object storage. Our approach enables scientists to scale from coarse-grained to fine-grained resolution and from a small to a larger region of interest. Using our empirical observations, we generate a cost model for the execution of workflows with hidden intermediate data on cloud services.},\n  keywords = {Cloud computing,Cloud service,Clouds,Cost model,Costs,Focusing,High-performance computing,Intermediate data,Object storage,Scalability,Soil measurements,Soil moisture,Workflows}\n}\n\n@inproceedings{10.1109/eScience55777.2022.00039,\n  title = {Enabling {{Call Path Querying}} in {{Hatchet}} to {{Identify Performance Bottlenecks}} in {{Scientific Applications}}},\n  booktitle = {2022 {{IEEE}} 18th {{International Conference}} on E-{{Science}} (e-{{Science}})},\n  author = {Lumsden, Ian and Luettgau, Jakob and Lama, Vanessa and {Scully-Allison}, Connor and Brink, Stephanie and Isaacs, Katherine E. and Pearce, Olga and Taufer, Michela},\n  year = {2022},\n  month = oct,\n  pages = {256--266},\n  doi = {10.1109/eScience55777.2022.00039},\n  url = {https://doi.org/10.1109/eScience55777.2022.00039},\n  urldate = {2024-02-01},\n  abstract = {As computational science applications benefit from larger-scale, more heterogeneous high performance computing (HPC) systems, the process of studying their performance becomes increasingly complex. The performance data analysis library Hatchet provides some insights into this complexity, but is currently limited in its analysis capabilities. Missing capabilities include the handling of relational caller-callee data captured by HPC profilers. To address this shortcoming, we augment Hatchet with a Call Path Query Language that leverages relational data in the performance analysis of scientific applications. Specifically, our Query Language enables data reduction using call path pattern matching. We demonstrate the effectiveness of our Query Language in identifying performance bottlenecks and enhancing Hatchet's analysis capabilities through three case studies. In the first case study, we compare the performance of sequential and multi-threaded versions of the graph alignment application Fido. In doing so, we identify the existence of large memory inefficiencies in both versions. In the second case study, we examine the performance of MPI calls in the linear algebra mini-application AMG2013 when using MVAPICH and Spectrum-MPI. In doing so, we identify hidden performance losses in specific MPI functions. In the third case study, we illustrate the use of our Query Language in Hatchet's interactive visualization. In doing so, we show that our Query Language enables a simple and intuitive way to massively reduce profiling data.},\n  keywords = {Full stack,High performance computing,High Performance Computing,Libraries,Linear algebra,Message Passing,Performance analysis,Performance Analysis,Proteins,Query Language,Scientific Applications,Scientific computing}\n}\n\n@inproceedings{10.1109/UCC56403.2022.00011,\n  ids = {luettgauNSDFCatalogLightweightIndexing2022},\n  title = {{{NSDF-Catalog}}: {{Lightweight Indexing Service}} for {{Democratizing Data Delivery}}},\n  shorttitle = {{{NSDF-Catalog}}},\n  booktitle = {2022 {{IEEE}}/{{ACM}} 15th {{International Conference}} on {{Utility}} and {{Cloud Computing}} ({{UCC}})},\n  author = {Luettgau, Jakob and Scorzelli, Giorgio and Pascucci, Valerio and Tarcea, Glenn and Kirkpatrick, Christine R. and Taufer, Michela},\n  year = {2022},\n  month = dec,\n  pages = {1--10},\n  address = {Portland, Oregon, USA},\n  doi = {10.1109/UCC56403.2022.00011},\n  url = {https://doi.org/10.1109/UCC56403.2022.00011},\n  urldate = {2024-01-22},\n  abstract = {Across domains massive amounts of scientific data are generated. Because of the large volume of information, data discoverability is a challenge, especially for scientists who have not generated the data or are from other domains. As part of the NSF-funded National Science Data Fabric (NSDF) initiative, we developed a testbed to demonstrate that these boundaries to data discoverability can be overcome. In support of this effort, we identify the need for indexing large-amounts of scientific data across scientific domains. We propose NSDF-Catalog, a lightweight indexing service with minimal metadata that complements existing domain-specific and rich-metadata col-lections. NSDF-Catalog is designed to facilitate multiple related objectives within a flexible microservice to: (i) coordinate data movements and replication of data from origin repositories within the NSDF federation; (ii) build an inventory of existing scientific data to inform the design of next-generation cyberinfrastructure; and (iii) provide a suite of tools for discovery of datasets for cross-disciplinary research. Our service indexes scientific data at a fine-granularity at the file or object level to inform data distribution strategies and to improve the experience for users from the consumer perspective, with the goal of allowing end-to-end dataflow optimizations.}\n}\n\n@inproceedings{10.1145/3588195.3592989,\n  title = {Thicket: {{Seeing}} the Performance Experiment Forest for the Individual Run Trees},\n  booktitle = {Proceedings of the 32nd International {{ACM}} Symposium on High-Performance Parallel and Distributed Computing ({{HPDC}})},\n  author = {Brink, Stephanie and McKinsey, Michael and Boehme, David and Hawkins, W. Daryl and {Scully-Allison}, Connor and Lumsden, Ian and Burgess, Treece and Lama, Vanessa and Isaacs, Katherine E. and Luettgau, Jakob and {Michela Taufer} and Pearce, Olga},\n  year = {2023},\n  month = jun,\n  pages = {1--10},\n  publisher = {ACM},\n  address = {Orlando, Florida, USA},\n  doi = {10.1145/3588195.3592989},\n  url = {https://doi.org/10.1145/3588195.3592989}\n}\n\n@inproceedings{10.1145/3603166.3632136,\n  title = {{{NSDF-Services}}: {{Integrating Networking}}, {{Storage}}, and {{Computing Services}} into a {{Testbed}} for {{Democratization}} of {{Data Delivery}}},\n  shorttitle = {{{NSDF-Services}}},\n  booktitle = {Proceedings of the {{IEEE}}/{{ACM}} 16th {{International Conference}} on {{Utility}} and {{Cloud Computing}}},\n  author = {Luettgau, Jakob and Martinez, Heberth and Olaya, Paula and Scorzelli, Giorgio and Tarcea, Glenn and Lofstead, Jay and Kirkpatrick, Christine and Pascucci, Valerio and Taufer, Michela},\n  year = {2024},\n  month = apr,\n  series = {{{UCC}} '23},\n  pages = {1--10},\n  publisher = {Association for Computing Machinery},\n  address = {New York, NY, USA},\n  doi = {10.1145/3603166.3632136},\n  url = {https://doi.org/10.1145/3603166.3632136},\n  urldate = {2024-04-18},\n  abstract = {The lack of a readily accessible, tightly integrated data fabric connecting high-speed networking, storage, and computing services remains a critical barrier to the democratization of scientific discovery. To address this challenge, we are building National Science Data Fabric (NSDF), a holistic ecosystem to facilitate domain scientists in their daily research. NSDF comprises networking, storage, and computing services, as well as outreach initiatives. In this paper, we present a testbed integrating three services (i.e., networking, storage, and computing). We evaluate their performance. Specifically, we study the networking services and their throughput and latency with a focus on academic cloud providers; the storage services and their performance with a focus on data movement using file system mappers for both academic and commercial clouds; and computing orchestration services focusing on commercial cloud providers. We discuss NSDF's potential to increase scalability and usability as it decreases time-to-discovery across scientific domains.},\n  isbn = {979-8-4007-0234-1},\n  keywords = {/unread,cloud computing,data democratization,high-performance computing,perfsonar,XRootD}\n}\n\n@inproceedings{10.1145/3605573.3605639,\n  title = {Scalable {{Incremental Checkpointing}} Using {{GPU-Accelerated De-Duplication}}},\n  booktitle = {Proceedings of the 52nd {{International Conference}} on {{Parallel Processing}}},\n  author = {Tan, Nigel and Luettgau, Jakob and Marquez, Jack and Teranishi, Keita and Morales, Nicolas and Bhowmick, Sanjukta and Cappello, Franck and Taufer, Michela and Nicolae, Bogdan},\n  year = {2023},\n  month = sep,\n  series = {{{ICPP}} '23},\n  pages = {665--674},\n  publisher = {Association for Computing Machinery},\n  address = {New York, NY, USA},\n  doi = {10.1145/3605573.3605639},\n  url = {https://doi.org/10.1145/3605573.3605639},\n  urldate = {2024-01-30},\n  abstract = {Writing large amounts of data concurrently to stable storage is a typical I/O pattern of many HPC workflows. This pattern introduces high I/O overheads and results in increased storage space utilization especially for workflows that need to capture the evolution of data structures with high frequency as checkpoints. In this context, many applications, such as graph pattern matching, perform sparse updates to large data structures between checkpoints. For these applications, incremental checkpointing techniques that save only the differences from one checkpoint to another can dramatically reduce the checkpoint sizes, I/O bottlenecks, and storage space utilization. However, such techniques are not without challenges: it is non-trivial to transparently determine what data has changed since a previous checkpoint and assemble the differences in a compact fashion that does not result in excessive metadata. State-of-art data reduction techniques (e.g., compression and de-duplication) have significant limitations when applied to modern HPC applications that leverage GPUs: slow at detecting the differences, generate a large amount of metadata to keep track of the differences, and ignore crucial spatiotemporal checkpoint data redundancy. This paper addresses these challenges by proposing a Merkle tree-based incremental checkpointing method to exploit GPUs' high memory bandwidth and massive parallelism. Experimental results at scale show a significant reduction of the I/O overhead and space utilization of checkpointing compared with state-of-the-art incremental checkpointing and compression techniques.},\n  isbn = {979-8-4007-0843-5},\n  keywords = {Checkpointing,data versioning,de-duplication,GPU parallelization,incremental storage}\n}\n\n@inproceedings{zanonboitoDeepLookTemporal2025,\n  title = {A {{Deep Look Into}} the {{Temporal I}}/{{O Behavior}} of {{HPC Applications}}},\n  booktitle = {2025 {{IEEE International Parallel}} and {{Distributed Processing Symposium}} ({{IPDPS}})},\n  author = {Zanon Boito, Francieli and Teylo, Luan and Popov, Mihail and Jolivel, Th{\\'e}o and Tessier, Fran{\\c c}ois and Luettgau, Jakob and Monniot, Julien and Tarraf, Ahmad and Carneiro, Andr{\\'e} and Osthoff, Carla},\n  year = {2025},\n  url = {https://inria.hal.science/hal-04887809},\n  urldate = {2025-02-03},\n  abstract = {The increasing gap between compute and I/O speeds in high-performance computing (HPC) systems imposes the need for techniques to improve applications' I/O performance. Such techniques must rely on assumptions about I/O behavior in order to efficiently allocate I/O resources such as burst buffers, to schedule accesses to the shared parallel file system or to delay certain applications at the batch scheduler level to prevent contention, for instance. In this paper, we verify these common assumptions about I/O behavior, specifically about temporal behavior, using over 440,000 traces from real HPC systems. By combining traces from diverse systems, we characterize the behaviors observed in real HPC workloads. Among other findings, we show that I/O activity tends to last for a few seconds, and that periodic jobs are the minority, but responsible for a large portion of the I/O time. Furthermore, we make projections for the expected improvement yielded by popular approaches for I/O performance improvement. Our work provides valuable insights to everyone working to alleviate the I/O bottleneck in HPC.},\n  langid = {english},\n  keywords = {/unread,high-performance computing,parallel file systems,temporal I/O behavior,workload characterization}\n}\n"},98904:(e,n,a)=>{a.d(n,{Z:()=>t});const t="@inproceedings{10.1007/978-3-030-02465-9_2,\n  title = {Cost and {{Performance Modeling}} for {{Earth System Data Management}} and {{Beyond}}},\n  booktitle = {High {{Performance Computing}}},\n  author = {Luettgau, Jakob and Kunkel, Julian},\n  editor = {Yokota, Rio and Weiland, Mich{\\`e}le and Shalf, John and Alam, Sadaf},\n  year = {2018},\n  series = {Lecture {{Notes}} in {{Computer Science}}},\n  pages = {23--35},\n  publisher = {Springer Int. Publishing},\n  address = {Cham},\n  doi = {10.1007/978-3-030-02465-9_2},\n  abstract = {Current and anticipated storage environments confront domain scientist and data center operators with usability, performance and cost challenges. The amount of data upcoming system will be required to handle is expected to grow exponentially, mainly due to increasing resolution and affordable compute power. Unfortunately, the relationship between cost and performance is not always well understood requiring considerable effort for educated procurement. Within the Centre of Excellence in Simulation of Weather and Climate in Europe (ESiWACE) models to better understand cost and performance of current and future systems are being explored. This paper presents models and methodology focusing on, but not limited to, data centers used in the context of climate and numerical weather prediction. The paper concludes with a case study of alternative deployment strategies and outlines the challenges anticipating their impact on cost and performance. By publishing these early results, we would like to make the case to work towards standard models and methodologies collaboratively as a community to create sufficient incentives for vendors to provide specifications in formats which are compatible to these modeling tools. In addition to that, we see application for such formalized models and information in I/O related middleware, which are expected to make automated but reasonable decisions in increasingly heterogeneous data centers.},\n  isbn = {978-3-030-02465-9},\n  langid = {english},\n  keywords = {Cost,Data management,Earth systems,Storage,TCO}\n}\n\n@inproceedings{10.1007/978-3-319-67630-2_12,\n  title = {Simulation of {{Hierarchical Storage Systems}} for {{TCO}} and {{QoS}}},\n  booktitle = {High {{Performance Computing}}},\n  author = {Luettgau, Jakob and Kunkel, Julian},\n  editor = {Kunkel, Julian M. and Yokota, Rio and Taufer, Michela and Shalf, John},\n  year = {2017},\n  series = {Lecture {{Notes}} in {{Computer Science}}},\n  pages = {132--144},\n  publisher = {Springer International Publishing},\n  address = {Cham},\n  doi = {10.1007/978-3-319-67630-2_12},\n  abstract = {Due to the variety of storage technologies deep storage hierarchies turn out to be the most feasible choice to meet performance and cost requirements when handling vast amounts of data. Long-term archives employed by scientific users are mainly reliant on tape storage, as it remains the most cost-efficient option. Archival systems are often loosely integrated into the HPC storage infrastructure. In expectation of exascale systems and in situ analysis also burst buffers will require integration with the archive. Exploring new strategies and developing open software for tape systems is a hurdle due to the lack of affordable storage silos and availability outside of large organizations and due to increased wariness requirements when dealing with ultra-durable data. Lessening these problems by providing virtual storage silos should enable community-driven innovation and enable site operators to add features where they see fit while being able to verify strategies before deploying on production systems. Different models for the individual components in tape systems are developed. The models are then implemented in a prototype simulation using discrete event simulation. The work shows that the simulations can be used to approximate the behavior of tape systems deployed in the real world and to conduct experiments without requiring a physical tape system.},\n  isbn = {978-3-319-67630-2},\n  langid = {english},\n  keywords = {Hierarchical storage systems,Long-term archive,Modeling,Performance,Simulation,Tape,Total cost of ownership}\n}\n\n@inproceedings{10.1109/eScience55777.2022.00081,\n  title = {Reproducing and {{Extending Analytical Performance Models}} of {{Generalized Hierarchical Scheduling}}},\n  booktitle = {2022 {{IEEE}} 18th {{International Conference}} on E-{{Science}} (e-{{Science}})},\n  author = {Luettgau, Jakob and {Caino-Lores}, Silvina and Suarez, Kae and Ahn, Dong H. and Herbein, Stephen and Taufer, Michela},\n  year = {2022},\n  month = oct,\n  pages = {450--455},\n  doi = {10.1109/eScience55777.2022.00081},\n  url = {https://ieeexplore.ieee.org/document/9973677},\n  urldate = {2025-01-06},\n  abstract = {Workflows in High-Performance Computing (HPC) are rapidly changing towards more complex and large-scale workflows. In particular, high-throughput and ensemble workflows are becoming increasingly common. These workflows impose significant burden on current HPC scheduling systems which typically use slow, centralized schedulers. Generalized hierarchical scheduling (GHS) is a potential solution to face modern workflows but is not widely adopted in HPC yet. One difficulty hindering widespread adoption is the lack of performance models to configure and fit application requirements. The few existing models are often built on stick assumptions that can substantially reduce the analysis realism. In this paper, we reproduce the analysis and improve the realism of a state-of-the-art model presented in ``An Analytical Performance Model of Generalized Hierarchical Scheduling'' [1] by Herbein and co-authors. Specifically, we first reproduce four key analysis studies in the original paper and then expand the model by removing key assumptions, one at a time. In doing so, we extend the realism of the original model. We empirically validate our extended model using three different scenarios and discuss the observed accuracy.},\n  keywords = {/unread,Adaptation models,Analytical models,Computational modeling,Ensemble workflows,Hierarchical scheduling,High performance computing,Performance modeling,Predictive models,Processor scheduling,Scalability,Workflow analysis}\n}\n\n@inproceedings{10.1109/PDSW-DISCS.2018.00012,\n  title = {Toward {{Understanding I}}/{{O Behavior}} in {{HPC Workflows}}},\n  booktitle = {2018 {{IEEE}}/{{ACM}} 3rd {{International Workshop}} on {{Parallel Data Storage}} \\& {{Data Intensive Scalable Computing Systems}} ({{PDSW-DISCS}})},\n  author = {Luettgau, Jakob and Snyder, Shane and Carns, Philip and Wozniak, Justin M. and Kunkel, Julian and Ludwig, Thomas},\n  year = {2018},\n  month = nov,\n  pages = {64--75},\n  address = {Dallas, TX, USA},\n  doi = {10.1109/PDSW-DISCS.2018.00012},\n  abstract = {Scientific discovery increasingly depends on complex workflows consisting of multiple phases and sometimes millions of parallelizable tasks or pipelines. These workflows access storage resources for a variety of purposes, including preprocessing, simulation output, and postprocessing steps. Unfortunately, most workflow models focus on the scheduling and allocation of computational resources for tasks while the impact on storage systems remains a secondary objective and an open research question. I/O performance is not usually accounted for in workflow telemetry reported to users. In this paper, we present an approach to augment the I/O efficiency of the individual tasks of workflows by combining workflow description frameworks with system I/O telemetry data. A conceptual architecture and a prototype implementation for HPC data center deployments are introduced. We also identify and discuss challenges that will need to be addressed by workflow management and monitoring systems for HPC in the future. We demonstrate how real-world applications and workflows could benefit from the approach, and we show how the approach helps communicate performance-tuning guidance to users.},\n  keywords = {Data models,Engines,HPC,I/O,Instrumentation,Monitoring,Pipelines,Storage,Task analysis,Telemetry,Tools,Visualization,Workflow}\n}\n\n@inproceedings{10.1109/PDSW.2014.9,\n  title = {Feign: {{In-Silico Laboratory}} for {{Researching I}}/{{O Strategies}}},\n  shorttitle = {Feign},\n  booktitle = {2014 9th {{Parallel Data Storage Workshop}}},\n  author = {L{\\\"u}ttgau, Jakob and Kunkel, Julian M.},\n  year = {2014},\n  month = nov,\n  pages = {43--48},\n  address = {New Orleans, LA, USA},\n  doi = {10.1109/PDSW.2014.9},\n  abstract = {Evaluating I/O performance of an application across different systems is a daunting task because it requires preparation of the software dependencies and required input data. Feign aims to be an extensible trace replay solution for parallel applications that supports arbitrary software and library layers. The tool abstracts and streamlines the replay process while allowing plug-ins to provide, manipulate and interpret trace data. Therewith, the application's behavior can be evaluated without potentially proprietary or confidential software and input data.Even more interesting is the potential of Feign as a virtual laboratory for I/O research: by manipulating trace data, experiments can be conducted; for example, it becomes possible to evaluate the benefit of optimization strategies. Since a plug-in could determine \"future\" activities, this enables us to develop optimal strategies as baselines for any run-time heuristics, but also eases testing of a developed strategy on many applications without modifying them.The paper proposes and evaluates a workflow to automatically apply optimization candidates to application traces and approximate potential performance gains. By using Feign's reporting facilities, an automatic optimization engine can then independently conduct experiments by feeding traces and strategies to compare the results.},\n  keywords = {Benchmark testing,Engines,Kernel,Laboratories,Optimization,Pipelines}\n}\n\n@inproceedings{10.1145/3503646.3524294,\n  title = {Data-{{Aware Compression}} for {{HPC}} Using {{Machine Learning}}},\n  booktitle = {Proceedings of the {{Workshop}} on {{Challenges}} and {{Opportunities}} of {{Efficient}} and {{Performant Storage Systems}}},\n  author = {Plehn, Julius and Fuchs, Anna and Kuhn, Michael and L{\\\"u}ttgau, Jakob and Ludwig, Thomas},\n  year = {2022},\n  month = apr,\n  series = {{{CHEOPS}} '22},\n  pages = {8--15},\n  publisher = {Association for Computing Machinery},\n  address = {New York, NY, USA},\n  doi = {10.1145/3503646.3524294},\n  url = {https://doi.org/10.1145/3503646.3524294},\n  urldate = {2022-07-11},\n  abstract = {While compression can provide significant storage and cost savings, its use within HPC applications is often only of secondary concern. This is in part due to the inflexibility of existing approaches where a single compression algorithm has to be used throughout the whole application but also because insights into the behaviour of the algorithms within the context of individual applications are missing. There are several different compression algorithms available, with each one also having a unique set of options. These options have a direct influence on the achieved performance and compression results. Furthermore, the algorithms and options to use for a given dataset are highly dependent on the characteristics of said dataset. This paper explores how machine learning can help with identifying fitting compression algorithms with corresponding options based on actual data structure encountered during I/O. In order to do so, a data collection and training pipeline is introduced. Inferencing is performed during regular application runs and shows promising results. Moreover, it provides valuable insights into the benefits of using certain compression algorithms and options for specific data. Further investigations into more advanced machine learning techniques and a deeper integration into existing I/O paths will provide additional benefits.},\n  isbn = {978-1-4503-9209-9},\n  keywords = {compression,file systems,HDF5,machine learning}\n}\n\n@inproceedings{10.1145/3624062.3624207,\n  title = {Enabling {{Agile Analysis}} of {{I}}/{{O Performance Data}} with {{PyDarshan}}},\n  booktitle = {Proceedings of the {{SC}} '23 {{Workshops}} of {{The International Conference}} on {{High Performance Computing}}, {{Network}}, {{Storage}}, and {{Analysis}}},\n  author = {Luettgau, Jakob and Snyder, Shane and Reddy, Tyler and Awtrey, Nikolaus and Harms, Kevin and Bez, Jean Luca and Wang, Rui and Latham, Rob and Carns, Philip},\n  year = {2023},\n  month = nov,\n  series = {{{SC-W}} '23},\n  pages = {1380--1391},\n  publisher = {Association for Computing Machinery},\n  address = {New York, NY, USA},\n  doi = {10.1145/3624062.3624207},\n  url = {https://doi.org/10.1145/3624062.3624207},\n  urldate = {2023-12-02},\n  abstract = {Modern scientific applications utilize numerous software and hardware layers to efficiently access data. This approach poses a challenge for I/O optimization because of the need to instrument and correlate information across those layers. The Darshan characterization tool seeks to address this challenge by providing efficient, transparent, and compact runtime instrumentation of many common I/O interfaces. It also includes command-line tools to generate actionable insights and summary reports. However, the extreme diversity of today's scientific applications means that not all applications are well served by one-size-fits-all analysis tools. In this work we present PyDarshan, a Python-based library that enables agile analysis of I/O performance data. PyDarshan caters to both novice and advanced users by offering ready-to-use HTML reports as well as a rich collection of APIs to facilitate custom analyses. We present the design of PyDarshan and demonstrate its effectiveness in four diverse real-world analysis use cases.},\n  isbn = {979-8-4007-0785-8},\n  keywords = {High-Performance Computing,Input/Output,Performance Analysis,Storage}\n}\n"},50400:(e,n,a)=>{a.d(n,{Z:()=>t});const t='@inproceedings{10.1109/eScience55777.2022.00068,\n  title = {Ubique: {{A New Model}} for {{Untangling Inter-task Data Dependence}} in {{Complex HPC Workflows}}},\n  shorttitle = {Ubique},\n  booktitle = {2022 {{IEEE}} 18th {{International Conference}} on E-{{Science}} (e-{{Science}})},\n  author = {Yeom, Jae-Seung and Ahn, Dong H. and Lumsden, Ian and Luettgau, Jakob and {Caino-Lores}, Silvina and Taufer, Michela},\n  year = {2022},\n  month = oct,\n  pages = {421--422},\n  doi = {10.1109/eScience55777.2022.00068},\n  url = {https://ieeexplore.ieee.org/document/9973591},\n  urldate = {2025-01-05},\n  abstract = {Exploiting task parallelism is getting increasingly difficult for diverse and complex scientific workflows running on High Performance Computing (HPC) systems. In this paper, we argue that the difficulty rises from a void in the spectrum of existing data-transfer models for resolving inter-task data dependence within a workflow and propose a novel model to fill that gap: Ubique. The Ubique model combines the best from in-transit and in situ models in order for loosely coupled producer and consumer tasks to run concurrently and to resolve their data dependencies efficiently with little or no modifications to their codes, striking a balance between transparent optimization, productivity, and performance. Our preliminary evaluation suggests that Ubique can significantly outperform the parallel file system (PFS)-based model while offering automatic data transfer and synchronization which are the features lacking in many traditional models. It also identifies the performance characteristics of its key depending subsystems, which must be understood for further broadening its benefits.},\n  keywords = {/unread,Computational modeling,Data models,Data transfer,File systems,High performance computing,in situ,in transit,job scheduling,Parallel processing,Productivity,workflow}\n}\n\n@inproceedings{10.1145/3502181.3533709,\n  ids = {olayaNSDFFUSETestbedStudying2022},\n  title = {{{NSDF-FUSE}}: {{A Testbed}} for {{Studying Object Storage}} via {{FUSE File Systems}}},\n  shorttitle = {{{NSDF-FUSE}}},\n  booktitle = {Proceedings of the 31st {{International Symposium}} on {{High-Performance Parallel}} and {{Distributed Computing}}},\n  author = {Olaya, Paula and Luettgau, Jakob and Zhou, Naweiluo and Lofstead, Jay and Scorzelli, Giorgio and Pascucci, Valerio and Taufer, Michela},\n  year = {2022},\n  month = jun,\n  series = {{{HPDC}} \'22},\n  pages = {277--278},\n  publisher = {Association for Computing Machinery},\n  address = {New York, NY, USA},\n  doi = {10.1145/3502181.3533709},\n  url = {https://doi.org/10.1145/3502181.3533709},\n  urldate = {2022-07-11},\n  abstract = {This work presents NSDF-FUSE, a testbed for evaluating settings and performance of FUSE-based file systems on top of S3-compatible object storage; the testbed is part of a suite of services from the National Science Data Fabric (NSDF) project (an NSF-funded project that is delivering cyberinfrastructures for data scientists). We demonstrate how NSDF-FUSE can be deployed to evaluate eight different mapping packages that mount S3-compatible object storage to a file system, as well as six data patterns representing different I/O operations on two cloud platforms. NSDF-FUSE is open-source and can be easily extended to run with other software mapping packages and different cloud platforms.},\n  isbn = {978-1-4503-9199-3},\n  keywords = {cloud,file system,fuse,object storage,performance}\n}\n\n@inproceedings{10.1145/3502181.3533710,\n  ids = {luettgauNSDFCloudEnablingAdHoc2022a,luettgauNSDFCloudEnablingAdHoc2022b,luettgauNSDFCloudEnablingAdHoc2022c},\n  title = {{{NSDF-Cloud}}: {{Enabling Ad-Hoc Compute Clusters Across Academic}} and {{Commercial Clouds}}},\n  shorttitle = {{{NSDF-Cloud}}},\n  booktitle = {Proceedings of the 31st {{International Symposium}} on {{High-Performance Parallel}} and {{Distributed Computing}}},\n  author = {Luettgau, Jakob and Olaya, Paula and Zhou, Naweiluo and Scorzelli, Giorgio and Pascucci, Valerio and Taufer, Michela},\n  year = {2022},\n  month = jun,\n  pages = {279--280},\n  publisher = {ACM},\n  address = {Minneapolis MN USA},\n  doi = {10.1145/3502181.3533710},\n  url = {https://dl.acm.org/doi/10.1145/3502181.3533710},\n  urldate = {2023-02-15},\n  isbn = {978-1-4503-9199-3},\n  langid = {english},\n  keywords = {cyberinfrastructure,data fabric,virtual machine}\n}\n\n@inproceedings{10.1145/3588195.3595948,\n  ids = {luettgau_hpdc23},\n  title = {Studying {{Latency}} and {{Throughput Constraints}} for {{Geo-Distributed Data}} in the {{National Science Data Fabric}}},\n  booktitle = {Proceedings of the 32nd {{International Symposium}} on {{High-Performance Parallel}} and {{Distributed Computing}}},\n  author = {Luettgau, Jakob and Martinez, Heberth and Tarcea, Glenn and Scorzelli, Giorgio and Pascucci, Valerio and Taufer, Michela},\n  year = {2023},\n  month = aug,\n  series = {{{HPDC}} \'23},\n  pages = {325--326},\n  publisher = {Association for Computing Machinery},\n  address = {New York, NY, USA},\n  doi = {10.1145/3588195.3595948},\n  url = {https://dl.acm.org/doi/10.1145/3588195.3595948},\n  urldate = {2024-01-22},\n  abstract = {The National Science Data Fabric (NSDF) is our solution to the problem of addressing the data-sharing needs of the growing data science community. NSDF is designed to make sharing data across geographically distributed sites easier for users who lack technical expertise and infrastructure. By developing an easy-to-install software stack, we promote the FAIR data-sharing principles in NSDF while leveraging existing high-speed data transfer infrastructures such as Globus and XRootD. This work shows how we leverage latency and throughput information between geo-distributed NSDF sites with NSDF entry points to optimize the automatic coordination of data placement and transfer across the data fabric, which can further improve the efficiency of data sharing.},\n  isbn = {979-8-4007-0155-9},\n  keywords = {cloud computing,data democratization,high-performance computing,perfsonar,xrootd}\n}\n\n@inproceedings{ATSFNAHLBP17,\n  title = {Adaptive {{Tier Selection}} for {{NetCDF}} and {{HDF5}}},\n  booktitle = {Supercomputing {{Conference}} 2017 ({{SC17}})},\n  author = {Luettgau, Jakob and Betke, Eugen and Perevalova, Olga and Kunkel, Julian and Kuhn, Michael},\n  year = {2017},\n  month = nov,\n  address = {Denver, CO, USA}\n}\n\n@misc{ICKKLLTKS15,\n  title = {Interaktiver {{C Kurs}} ({{ICP}})},\n  author = {Kunkel, Julian and Ludwig, Thomas and L{\\"u}ttgau, Jakob and Timmermann, Dion and Kautz, Christian and Skwarek, Volker},\n  year = {2015},\n  month = nov,\n  url = {http://wr.informatik.uni-hamburg.de/_media/research/projects/icp/hoou-2016-poster.pdf},\n  abstract = {Programmiersprachen bilden die Basis f{\\"u}r die automatisierte Datenverarbeitung in der digitalen Welt. Obwohl die Grundkonzepte einfach zu verstehen sind, beherrscht nur ein geringer Anteil von Personen diese Werkzeuge. Die Gr{\\"u}nde hierf{\\"u}r sind Defizite in der Ausbildung und die Einstiegssh{\\"u}rde bei der Bereitstellung einer produktiven Programmierumgebung. Insbesondere erfordert das Erlernen einer Programmiersprache die praktische Anwendung der Sprache. Eine Integration von Programmierkursen in die Hamburg Open Online University verbessert nicht nur das Angebot f{\\"u}r Studierende, sondern erschlie{\\ss}t auch Fachfremden den Zugang zur Informatik.}\n}\n\n@inproceedings{kunkelMiddlewareEarthSystem2016,\n  title = {Middleware for {{Earth System Data}}},\n  author = {Kunkel, Julian and Luettgau, Jakob and Lawrence, Bryan N and Jensen, Jens and Congiu, Giuseppe and Readey, John},\n  year = {2016},\n  publisher = {1st Joint International Workshop on Parallel Data Storage \\& Data Intensive Scalable Computing Systems (PDSW-DISCS) WIPs},\n  abstract = {Making the best use of HPC in Earth simulation requires storing and manipulating vast quantities of data. Existing storage environments face usability and performance challenges for both domain scientists and the data centers supporting the scientists. These challenges arise from data discovery/access patterns, and the need to support complex legacy interfaces.},\n  langid = {english}\n}\n\n@inproceedings{luttgauDataChallengesClimate2021,\n  title = {Data {{Challenges}} for {{Climate Machine Learning Workflows}}},\n  booktitle = {{{GEOMAR}} 5th {{Data Science Symposium}} 2021},\n  author = {L{\\"u}ttgau, Jakob and Weigel, Tobias and Greenberg, David},\n  year = {2021},\n  volume = {5}\n}\n\n@inproceedings{luttgauStructureAwareEarthSystem2017,\n  title = {Towards {{Structure-Aware Earth System Data Management}}},\n  author = {L{\\"u}ttgau, Jakob and Kunkel, Julian and Lawrence, Bryan N. and Fiore, Sandro and Hua, Huang},\n  year = {2017},\n  publisher = {2nd Joint International Workshop on Parallel Data Storage \\& Data Intensive Scalable Computing Systems (PDSW-DISCS) WIPs}\n}\n\n@inproceedings{MASOTLFHSS16,\n  title = {Modeling and {{Simulation}} of {{Tape Libraries}} for {{Hierarchical Storage Systems}}},\n  booktitle = {Supercomputing {{Computing}}  ({{SC16}})},\n  author = {L{\\"u}ttgau, Jakob and Kunkel, Julian},\n  year = {2016},\n  month = nov,\n  address = {Salt Lake City, Utah, USA},\n  url = {http://sc16.supercomputing.org/sc-archive/tech_poster/tech_poster_pages/post123.html},\n  activity = {SC16}\n}\n\n@inproceedings{uhleRealtimeDetectionsVolcanic2019,\n  title = {Real-Time Detections of Volcanic Eruptions Using Neuronal Networks},\n  booktitle = {79. {{Jahrestagung Deutsche Geophysikalische Gesellschaft}}},\n  author = {Uhle, Daniel and Hort, M. and L{\\"u}ttgau, J. and Walda, J.},\n  year = {2019},\n  month = mar,\n  address = {Braunschweig, Germany},\n  url = {https://docplayer.org/193864623-79-jahrestagung-deutsche-geophysikalische-gesellschaft-maerz-2019-in-braunschweig-tagungsprogramm.html},\n  urldate = {2022-08-02},\n  abstract = {M{\\"a}rz 2019 in Braunschweig Tagungsprogramm ISSN Deutsche Nationalbibliothek Bibliographische Daten unter}\n}\n\n@inproceedings{weigelAIEarthSystem2021,\n  title = {{{AI}} for {{Earth System Sciences}}: {{Bottlenecks}}, {{Pitfalls}} and {{Recommendations}}},\n  booktitle = {Platform for {{Advanced Scientific Computing}} ({{PASC21}})},\n  author = {Weigel, Tobias and L{\\"u}ttgau, Jakob and Kadow, Christopher and Greenberg, David and Bouwer, Laurens M. and Bockelmann, Hendryk and Schrum, Corinna and Ludwig, Thomas},\n  year = {2021},\n  month = jul,\n  publisher = {PASC21},\n  url = {https://pasc21.pasc-conference.org/program/schedule/index.html%3Fpost_type=page&p=10&id=post143&sess=sess182.html},\n  isbn = {Geneva, Switzerland}\n}\n'},90234:(e,n,a)=>{a.d(n,{Z:()=>t});const t="@techreport{10.5281/zenodo.1228749,\n  title = {Business {{Model}} with {{Alternative Scenarios}} ({{D4}}.1)},\n  author = {Luettgau, Jakob and Kunkel, Julian and Jensen, Jens and Lawrence, Bryan},\n  year = {2017},\n  month = feb,\n  institution = {{Centre of Excellence for Weather and Climate in Europe (ESiWACE)}},\n  doi = {10.5281/zenodo.1228749},\n  url = {https://zenodo.org/record/1228749},\n  urldate = {2022-07-30},\n  abstract = {Abstract This report summarizes the work on requirements and business models for the storage infrastructure within weather and climate data centres (although much of the work has wider applicability).~ The report concentrates on identifying and evaluating the interplay of important cost factors, along with an introduction to relevant (storage and data movement) hardware and software technology,~ terminology and performance metrics. The report begins with a description of how climate and weather applications make use of HPC systems, the arising challenges and data requirements, and some trends that are likely to impact future data centre designs. Related work follows that introduces some cost developments, cost modelling and technological developments. The body of the work is an integrated graph-based approach to modelling costs, resilience and performance for storage systems. Storage models are evaluated in several scenarios each introducing some architectural changes to currently deployed high-performance systems and discusses the cost and performance implications. These discussions are made on a high-level of abstraction as no model is able to predict the non-linear behaviour when scaling out big systems accurately. Conclusions identify the potential benefits that more refined models might offer and outline future work. ~ About this document Work package in charge: WP4 Exploitability Actual delivery date for this deliverable: 24 February 2017 Dissemination level: PU (for public use) Lead author:~ Deutsches Klimarechenzentrum (DKRZ), ~Jakob Luettgau Other contributing authors: Deutsches Klimarechenzentrum (DKRZ),~ Julian Kunkel, Jakob Luettgau Science and Technology Facilities Council (STFC), Jens Jensen The University of Reading (UREAD), and Science and Technology Facilities Council (STFC), Bryan Lawrence},\n  langid = {english},\n  keywords = {business models,climate data centre,data centre design,HPC,performance implications.,storage and data movement,storage infrastructure}\n}\n\n@techreport{10.5281/zenodo.1453895,\n  title = {Operational {{Demonstrator}} of {{ESD Middleware}} ({{MS5}})},\n  author = {Kunkel, Julian and Luettgau, Jakob and Betke, Eugen},\n  year = {2017},\n  month = aug,\n  institution = {{Centre of Excellence for Weather and Climate in Europe (ESiWACE)}},\n  doi = {10.5281/zenodo.1453895},\n  url = {https://zenodo.org/record/1453895},\n  urldate = {2022-07-30},\n  abstract = {The demonstrator consists of three parts: 1) A typical I/O workload that is mimicking checkpoint/restart, 2) a HDF5 VOL Plugin to redirect and persist data to different storage targets and 3) a policy engine that fetches runtime information and adapts the storage target accordingly.},\n  langid = {english}\n}\n\n@techreport{10.5281/zenodo.1453902,\n  title = {Prototypes of {{Alternative Storage Backends}} ({{MS7}})},\n  author = {Kunkel, Julian and Luettgau, Jakob},\n  year = {2018},\n  month = may,\n  institution = {{Centre of Excellence for Weather and Climate in Europe (ESiWACE)}},\n  doi = {10.5281/zenodo.1453902},\n  url = {https://zenodo.org/record/1453902},\n  urldate = {2022-07-30},\n  abstract = {The objectives of the Exploitability work package within ESiWACE include two objectives: (1) The development of a new way of laying data onto storage which provides performance without compromising the familiar NetCDF interface. (2) The development of a prototype tape library. We are delivering these two objectives using the same basic philosophy, which is to develop middleware which is easily deployable by users and system administrators, and which utilises fragmentation to provide performance. The current design includes the same basic concepts in two different software packages (the ``Earth System Data Middleware'' and the ``Semantic Storage Layer'') which target these two objectives. Future projects may merge some or all the underlying functionality. This milestone (7) reports a staging point in the development of these two components, the release of tagged software for the ESDM and SemSL.}\n}\n\n@misc{10.5281/zenodo.14605692,\n  title = {I/{{O}} and {{Storage}} -  {{European Strategic Research Agenda}} 6 ({{White Paper}})},\n  author = {{Sarah Neuwirth} and {Philippe Deniel} and {Jean-Thomas Acquaviva} and {Martin Golasowski} and {Michael Hennecke} and {Adrian Jackson} and {Thomas Leibovici} and {Jakob Luettgau} and {Ramon Nou}},\n  year = {2025},\n  month = jan,\n  number = {SRA 6},\n  eprint = {SRA 6},\n  publisher = {European Technology Platform for High Performance Computing (ETP4HPC)},\n  doi = {10.5281/zenodo.14605692},\n  url = {https://etp4hpc.eu/download/61/white-papers/5497/etp4hpc_wp_io-storage_20250106.pdf},\n  archiveprefix = {European Technology Platform for High Performance Computing (ETP4HPC)}\n}\n\n@techreport{10.5281/zenodo.2573896,\n  title = {New {{Storage Layout}} for {{Earth System Data}} ({{D4}}.2)},\n  author = {Luettgau, Jakob and Kunkel, Julian and Lawrence, Bryan and Jensen, Jens and Congiu, {\\relax Gi}useppe and Hua, Huang and Nassisi, Paola},\n  year = {2017},\n  month = jul,\n  institution = {{Centre of Excellence for Weather and Climate in Europe (ESiWACE)}},\n  doi = {10.5281/zenodo.2573896},\n  url = {https://zenodo.org/record/2573896},\n  urldate = {2022-07-30},\n  abstract = {Abstract Making the best use of HPC in Earth simulation requires storing and manipulating vast quantities of data. Existing storage environments face usability and performance challenges for both domain scientists and the data centers supporting the scientists. These challenges arise from data discovery/access patterns, and the need to support complex legacy interfaces. In the ESiWACE project, we develop a novel I/O middleware targeting, but not limited to, earth system data. This deliverable sheds light upon the technical design of the ESD middleware, and the user perspective and implications when using the middleware. Its architecture builds on well-established end-user interfaces but utilizes scientific metadata to harness a data structure centric perspective. In contrast to existing solutions, the middleware maps data structures to available storage technology based on several parameters: 1) A data center specific configuration of available hardware with their characteristics; 2) The intended usage pattern explicitly provided by the user and implicitly by the structure of the data. This allows to exploit performance characteristics of a heterogeneous storage environment more efficiently. This deliverable provides the background on data representations and description formats commonly used in earth system modeling. The document isolates the key requirements for an earth system middleware and collects numerous use-case outlining the benefit to existing and anticipated workflows and technologies. Finally, a detailed initial design for the architecture of the earth system middleware is proposed and documented. The document is not intended to describe all components completely but provides a high-level overview that is necessary to build a first prototype as it is planned in the next phase of the ESiWACE project. During this development, the design will be adjusted to match the prototype; the final version of the design document will be delivered with the end of the project. ~ About this document Work package in charge: WP4 Exploitability Actual delivery date for this deliverable: July 2017 Dissemination level: PU Lead author:~ Deutsches Klimarechenzentrum GmbH (DKRZ), Jakob L{\\\"u}ttgau Other contributing authors: Deutsches Klimarechenzentrum GmbH (DKRZ), Julian Kunkel Science and Technology Facilities Council (STFC), Bryan Lawrence, Jens Jensen The University of Reading (UREAD), Bryan Lawrence Seagate Systems UK Limited (SEAGATE), Giuseppe Congiu, Huang Hua Fondazione Centro Euro-Mediterraneo sui Cambiamenti Climatici (CMCC): Paola Nassisi},\n  langid = {english},\n  keywords = {Earth simulation,earth system data,HPC,I/O middleware,storage environments}\n}\n\n@techreport{10.5281/zenodo.3551787,\n  title = {New {{Storage Layout}} for {{Earth System Data}} ({{D4}}.2) ({{Version}} 2.0)},\n  author = {Luettgau, Jakob and Kunkel, Julian and Lawrence, Bryan and Jensen, Jens and Congiu, Giuseppe and Hua, Huan and Paola, Nassisi},\n  year = {2019},\n  month = nov,\n  institution = {{Centre of Excellence for Weather and Climate in Europe (ESiWACE)}},\n  doi = {10.5281/zenodo.3551787},\n  url = {https://zenodo.org/record/3551787},\n  urldate = {2022-07-30},\n  abstract = {Making the best use of HPC in Earth simulation requires storing and manipulating vast quantities of data. Existing storage environments face usability and performance challenges for both domain scientists and the data centers supporting the scientists. These challenges arise from data discovery/access patterns, and the need to support complex legacy interfaces. In the ESiWACE project, we develop a novel I/O middleware targeting, but not limited to, earth system data. This deliverable sheds light upon the technical design of the ESD middleware, and the user perspective and implications when using the middleware. Its architecture builds on well-established end-user interfaces but utilizes scientific metadata to harness a data structure centric perspective. In contrast to existing solutions, the middleware maps data structures to available storage technology based on several parameters: 1) A data center specific configuration of available hardware with their characteristics; 2) The intended usage pattern explicitly provided by the user and implicitly by the structure of the data. This allows to exploit performance characteristics of a heterogeneous storage environment more efficiently. This deliverable provides the background on data representations and description formats commonly used in earth system modeling. The document isolates the key requirements for an earth system middleware and collects numerous use-case outlining the benefit to existing and anticipated workflows and technologies. Finally, a detailed initial design for the architecture of the earth system middleware is proposed and documented. The document is not intended to describe all components completely but provides a high-level overview that is necessary to build a first prototype as it is planned in the next phase of the ESiWACE project. During this development, the design will be adjusted to match the prototype; the final version of the design document will be delivered with the end of the project.},\n  langid = {english},\n  keywords = {HPC Earth simulation storage environments I/O middleware earth system data}\n}\n\n@techreport{WRSCABCFGJ17,\n  title = {Wissenschaftliches {{Rechnen}} - {{Scientific Computing}} - 2016},\n  author = {Alforov, Yevhen and Betke, Eugen and Chasapis, Konstantinos and Fuchs, Anna and Gro{\\ss}e, Fabian and Jumah, Nabeeh and Kuhn, Michael and Kunkel, Julian and Lenhart, Hermann and L{\\\"u}ttgau, Jakob and Neumann, Philipp and Novikova, Anastasiia and Squar, Jannek and Ludwig, Thomas},\n  year = {2017},\n  month = jun,\n  address = {Deutsches Klimarechenzentrum GmbH, Hamburg, Germnay},\n  institution = {Research Group: Scientific Computing, University of Hamburg}\n}\n"},72984:(e,n,a)=>{a.d(n,{Z:()=>t});const t='@phdthesis{luttgauDecisionSupportWorkflowAware2021,\n  type = {Dissertation},\n  title = {Decision {{Support}} for {{Workflow-Aware High-Performance Storage Systems}}},\n  author = {L{\\"u}ttgau, Jakob},\n  year = {2021},\n  month = may,\n  address = {Hamburg},\n  url = {https://jakobluettgau.com/static/theses/luettgau_decision-support-for-workflow-aware-high-performance-storage-systems.pdf},\n  langid = {english},\n  school = {Universit{\\"a}t Hamburg}\n}\n\n@phdthesis{luttgauFlexibleEventImitation2014,\n  type = {Bachelor\'s {{Thesis}}},\n  ids = {FEIEFPWL14},\n  title = {Flexible Event Imitation Engine for Parallel Workloads},\n  author = {L{\\"u}ttgau, Jakob},\n  year = {2014},\n  month = mar,\n  address = {Hamburg},\n  url = {https://jakobluettgau.com/static/theses/jakob_luettgau_flexible_event_imitation_engine_for_parallel_workloads.pdf},\n  abstract = {Evaluating systems and optimizing applications in high-performance computing (HPC) is a tedious task. Trace files, which are already commonly used to analyse and tune applications, also serve as a good approximation to reproduce workloads of scientific applications. The thesis presents design considerations and discusses a prototype implementation for a flexible tool to mimic the behavior of parallel applications by replaying trace files. In the end it is shown that a plugin based replay engine is able to replay parallel workloads that use MPI and POSIX I/O. It is further demonstrated how automatic trace manipulation in combination with the replay engine allows to be used as a virtual lab.},\n  advisors = {Julian Kunkel},\n  howpublished = {Online \\{\\{:research:theses:jakob\\_l\\_\\_ttgau\\_flexible\\_event\\_imitation\\_engine\\_for\\_parallel\\_workloads.pdf{\\textbar}Thesis\\}\\}},\n  langid = {english},\n  school = {Universit{\\"a}t Hamburg}\n}\n\n@mastersthesis{MASOTLFHSM16,\n  type = {Master\'s {{Thesis}}},\n  title = {Modeling and Simulation of Tape Libraries for Hierarchical Storage Management Systems},\n  author = {L{\\"u}ttgau, Jakob},\n  year = {2016},\n  month = sep,\n  address = {Hamburg},\n  url = {https://jakobluettgau.com/static/theses/jakob_luettgau_modeling_and_simulation_of_tape_libraries_for_hierarchical_storage_management_systems.pdf},\n  abstract = {The wide variety of storage technologies (SRAM, NVRAM, NAND, Disk, Tape, etc.) results in deep storage hierarchies to be the only feasible choice to meet performance and cost requirements when dealing with vast amounts of data. In particular long term storage systems employed by scientific users are mainly reliant on tape storage, as they are still the most cost-efficient option even 40 years after their invention in the mid-seventies. Current archival systems are often loosely integrated into the remaining HPC storage infrastructure. However, data analysis tasks require the integration into the scratch storage systems. With the rise of exascale systems and in situ analysis also burst buffers are likely to require integration with the archive. Unfortunately, exploring new strategies and developing open software for tape archive systems is a hurdle due to the lack of affordable storage silos, the resulting lack of availability outside of large organizations and due to increased wariness requirements when dealing with ultra durable data. Eliminating some of these problems by providing virtual storage silos should enable community-driven innovation, and enable site operators to add features where they see fit while being able to verify strategies before deploying on test or production systems. The thesis asseses moderns tape systems and also puts their development over time into perspective. Subsequently, different models for the individual components in tape systems are developed. The models are then implemented in a prototype simulation using discrete event simulation. It is shown that the simulation can be used to approximate the behavior of tape systems deployed in the real world and to conduct experiments without requiring a physical tape system.},\n  advisors = {Julian Kunkel},\n  howpublished = {Online https://wr.informatik.uni-hamburg.de/\\_media/research:theses:jakob\\_luettgau\\_modeling\\_and\\_simulation\\_of\\_tape\\_libraries\\_for\\_hierarchical\\_storage\\_management\\_systems.pdf},\n  langid = {english},\n  school = {Universit{\\"a}t Hamburg}\n}\n'}}]);