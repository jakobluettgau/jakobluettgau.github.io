(globalThis["webpackChunkquasar_tufte"]=globalThis["webpackChunkquasar_tufte"]||[]).push([[431],{29431:(e,n,a)=>{"use strict";a.r(n),a.d(n,{default:()=>Z});var t=a(59835),i=a(86970),r=a(51611),o=a.n(r);const s=e=>((0,t.dD)("data-v-3ba34cf8"),e=e(),(0,t.Cn)(),e),l={class:"tufte-body"},c=(0,t.Uk)(),d=(0,t.Uk)(),h=(0,t.Uk)(),u=s((()=>(0,t._)("h2",null,"Overview",-1))),m=(0,t.Uk)(),p=(0,t.Uk)("\n  \tWe take 29 different machine learning approaches (some of which host many variants within) nd feed them into a knowledge graph. \n The resulting graph is shown in "),g=(0,t.Uk)(".\n We track 5 common machine learning tasks (there are certainly many more that could be considered), namely, Classification, Regression, Generative Models, Anomaly Detection, and Clustering.\n  \tWe also follow the common categorization into supervised and unsupervised approaches. In addition to that we track if the approaches fall into the category of neural networks, reinforcment learning. \nWe also mention Genetic Algorithms as a way to cope with unfeasibly large search spaces, which in principle can be coupled with many of the approaches.\nThis notion of hybrid approaches i becoming more and more relevant in any case, as many advanced machine learning systems combine the properties of different approaches to solve tasks in increasingly complex environements.\n  "),f=(0,t.Uk)(),b=s((()=>(0,t._)("img",{src:o()},null,-1))),w=(0,t.Uk)(),v=(0,t.Uk)("29 machine learning approaches mapped using relations in a graph structure and visualized using a spring layout. Highlighted in green are neural approaches and decision trees."),y=(0,t.Uk)(),k=(0,t.Uk)('\nThe map uses many abbreviations.\nWhere available the name used in the original work or publication is used.\nMany of the abbreviations for neural approaches are based on the taxonomy used in "The Neural Network Zoo" '),T=(0,t.Uk)(". \nAll abbreviations are resolved as follows:\n\nAuto Encoder (AE) / Variational Auto Encoder (VAE) "),x=(0,t.Uk)(";\nDeep Q Networks (DQN) "),N=(0,t.Uk)(";\nFeed Forward Neural Network (FFNN) "),A=(0,t.Uk)(";\nConvolutional Neural Networks (CNN) "),M=(0,t.Uk)(";\nLong Short-Term Memory (LSTM) "),S=(0,t.Uk)(";\nGenerative Adversarial Network (GAN) "),U=(0,t.Uk)(";\nState–action–reward–state–action (SARSA) "),W=(0,t.Uk)(";\nSelf-Organizing Map (SOM) "),L=(0,t.Uk)(";\nRecurrent Neural Network (RNN) "),z=(0,t.Uk)(),E=(0,t.Uk)(".\n"),G=(0,t.Uk)(),D={key:0},_=s((()=>(0,t._)("h3",{class:"no-count"},"References",-1))),B=(0,t.Uk)(),C={class:"fullwidth"};function j(e,n,a,r,o,s){const j=(0,t.up)("blog-metadata"),q=(0,t.up)("t-ref"),I=(0,t.up)("t-figure"),R=(0,t.up)("t-cite"),P=(0,t.up)("BibtexBibliography");return(0,t.wg)(),(0,t.iD)("div",l,[(0,t._)("article",null,[(0,t._)("section",null,[(0,t._)("h1",null,(0,i.zw)(e.metadata.title),1),c,(0,t.Wm)(j,{metadata:e.metadata},null,8,["metadata"]),d,(0,t._)("p",null,(0,i.zw)(e.metadata.text),1)]),h,(0,t._)("section",null,[u,m,(0,t._)("p",null,[p,(0,t.Wm)(q,{label:"fig:map-of-machine learning approaches"}),g]),f,(0,t.Wm)(I,{label:"fig:map-of-machine learning approaches"},{caption:(0,t.w5)((()=>[v])),default:(0,t.w5)((()=>[b,w])),_:1}),y,(0,t._)("p",null,[k,(0,t.Wm)(R,{name:"veenNeuralNetworkZoo2016"}),T,(0,t.Wm)(R,{name:"kingmaAutoEncodingVariationalBayes2014"}),x,(0,t.Wm)(R,{name:"mnihMethodsApparatusreinforcement2015"}),N,(0,t.Wm)(R,{name:"10.1037/h0042519"}),(0,t.Wm)(R,{name:"veenNeuralNetworkZoo2016"}),A,(0,t.Wm)(R,{name:"10.1109/5.726791"}),(0,t.Wm)(R,{name:"veenNeuralNetworkZoo2016"}),M,(0,t.Wm)(R,{name:"10.1162/neco.1997.9.8.1735"}),S,(0,t.Wm)(R,{name:"goodfellowGenerativeAdversarialNetworks2014"}),U,(0,t.Wm)(R,{name:"rummeryOnlineQLearningusing1994"}),W,(0,t.Wm)(R,{name:"10.1109/5.58325"}),L,(0,t.Wm)(R,{name:"DBLP:journals/cogsci/Elman90"}),z,(0,t.Wm)(R,{name:"veenNeuralNetworkZoo2016"}),E])]),G,e.references?((0,t.wg)(),(0,t.iD)("section",D,[_,B,(0,t._)("p",C,[(0,t.Wm)(P,{bibtex:e.references},null,8,["bibtex"])])])):(0,t.kq)("",!0)])])}var q=a(60499),I={title:"Map of Machine Learning Approaches",authors:["Jakob Luettgau"],datetime:"2021-09-21T17:37:58Z",text:"The landscape of machine learning approahes is vast and only increases to grow as computational power is available. Without any aspiration to be complete, this post maps out a number of very established machine learning methods and relates them to each other. The resulting graph when using a spring-layout yields an interesting perspective that also intuitively appears appropriate.",image:"./svg/ml_nn-approaches-no-text.png",caption:null,published:!0},R=a(78225).Z;const P=(0,t.aZ)({name:"BlogPost",components:{},setup(){const e=(0,q.iH)(R);return{metadata:I,references:e}}});var O=a(11639);const F=(0,O.Z)(P,[["render",j],["__scopeId","data-v-3ba34cf8"]]),Z=F},78225:(e,n,a)=>{"use strict";a.d(n,{Z:()=>t});const t='@article{10.1037/h0042519,\n  title = {The Perceptron: {{A Probabilistic Model}} for {{Information Storage}} and {{Organization}} in the {{Brain}}},\n  shorttitle = {The Perceptron},\n  author = {Rosenblatt, Frank},\n  year = {1958},\n  journal = {Psychological Review},\n  volume = {65},\n  number = {6},\n  pages = {386--408},\n  issn = {1939-1471, 0033-295X},\n  doi = {10.1037/h0042519},\n  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/h0042519},\n  urldate = {2023-08-11},\n  langid = {english}\n}\n\n@article{10.1109/5.58325,\n  title = {The {{Self-Organizing Map}}},\n  author = {Kohonen, T.},\n  year = {1990},\n  month = sep,\n  journal = {Proceedings of the IEEE},\n  volume = {78},\n  number = {9},\n  pages = {1464--1480},\n  issn = {1558-2256},\n  doi = {10.1109/5.58325},\n  abstract = {The self-organized map, an architecture suggested for artificial neural networks, is explained by presenting simulation experiments and practical applications. The self-organizing map has the property of effectively creating spatially organized internal representations of various features of input signals and their abstractions. One result of this is that the self-organization process can discover semantic relationships in sentences. Brain maps, semantic maps, and early work on competitive learning are reviewed. The self-organizing map algorithm (an algorithm which order responses spatially) is reviewed, focusing on best matching cell selection and adaptation of the weight vectors. Suggestions for applying the self-organizing map algorithm, demonstrations of the ordering process, and an example of hierarchical clustering of data are presented. Fine tuning the map by learning vector quantization is addressed. The use of self-organized maps in practical speech recognition and a simulation experiment on semantic mapping are discussed.{$<>$}}\n}\n\n@article{10.1109/5.726791,\n  title = {Gradient-Based Learning Applied to Document Recognition},\n  author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},\n  year = {1998},\n  month = nov,\n  journal = {Proceedings of the IEEE},\n  volume = {86},\n  number = {11},\n  pages = {2278--2324},\n  issn = {1558-2256},\n  doi = {10.1109/5.726791},\n  abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.}\n}\n\n@article{10.1162/neco.1997.9.8.1735,\n  title = {Long {{Short-Term Memory}}},\n  author = {Hochreiter, Sepp and Schmidhuber, J{\\"u}rgen},\n  year = {1997},\n  month = nov,\n  journal = {Neural Computation},\n  volume = {9},\n  number = {8},\n  pages = {1735--1780},\n  issn = {0899-7667, 1530-888X},\n  doi = {10.1162/neco.1997.9.8.1735},\n  url = {https://direct.mit.edu/neco/article/9/8/1735-1780/6109},\n  urldate = {2021-05-29},\n  abstract = {Learning to store information over extended time intervals via recurrent backpropagation takes a very long time, mostly due to insu cient, decaying error back ow. We brie y review Hochreiter\'s 1991 analysis of this problem, then address it by introducing a novel, e cient, gradient-based method called {\\textbackslash}Long Short-Term Memory" (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete time steps by enforcing constant error ow through {\\textbackslash}constant error carrousels" within special units. Multiplicative gate units learn to open and close access to the constant error ow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with arti cial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with RTRL, BPTT, Recurrent Cascade-Correlation, Elman nets, and Neural Sequence Chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, arti cial long time lag tasks that have never been solved by previous recurrent network algorithms.},\n  langid = {english}\n}\n\n@article{bourlardAutoassociationMultilayerperceptrons2004,\n  title = {Auto-Association by Multilayer Perceptrons and Singular Value Decomposition},\n  author = {Bourlard, H. and Kamp, Y.},\n  year = {2004},\n  journal = {undefined},\n  url = {/paper/Auto-association-by-multilayer-perceptrons-and-Bourlard-Kamp/f5821548720901c89b3b7481f7500d7cd64e99bd},\n  urldate = {2021-05-29},\n  abstract = {The multilayer perceptron, when working in auto-association mode, is sometimes considered as an interesting candidate to perform data compression or dimensionality reduction of the feature space in information processing applications. The present paper shows that, for auto-association, the nonlinearities of the hidden units are useless and that the optimal parameter values can be derived directly by purely linear techniques relying on singular value decomposition and low rank matrix approximation, similar in spirit to the well-known Karhunen-Lo{\\`e}ve transform. This approach appears thus as an efficient alternative to the general error back-propagation algorithm commonly used for training multilayer perceptrons. Moreover, it also gives a clear interpretation of the r{\\^o}le of the different parameters.},\n  langid = {english}\n}\n\n@article{DBLP:journals/cogsci/Elman90,\n  title = {Finding Structure in Time},\n  author = {Elman, Jeffrey L.},\n  year = {1990},\n  journal = {Cogn. Sci.},\n  volume = {14},\n  number = {2},\n  pages = {179--211},\n  doi = {10.1207/s15516709cog1402\\_1},\n  url = {https://doi.org/10.1207/s15516709cog1402_1},\n  bibsource = {dblp computer science bibliography, https://dblp.org},\n  biburl = {https://dblp.org/rec/journals/cogsci/Elman90.bib},\n  timestamp = {Thu, 04 Jun 2020 19:37:03 +0200}\n}\n\n@article{goodfellowGenerativeAdversarialNetworks2014,\n  title = {Generative {{Adversarial Networks}}},\n  author = {Goodfellow, Ian J. and {Pouget-Abadie}, Jean and Mirza, Mehdi and Xu, Bing and {Warde-Farley}, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},\n  year = {2014},\n  month = jun,\n  journal = {arXiv:1406.2661 [cs, stat]},\n  eprint = {1406.2661},\n  primaryclass = {cs, stat},\n  url = {http://arxiv.org/abs/1406.2661},\n  urldate = {2021-05-29},\n  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},\n  archiveprefix = {arxiv}\n}\n\n@article{kingmaAutoEncodingVariationalBayes2014,\n  title = {Auto-{{Encoding Variational Bayes}}},\n  author = {Kingma, Diederik P. and Welling, Max},\n  year = {2014},\n  month = may,\n  journal = {arXiv:1312.6114 [cs, stat]},\n  eprint = {1312.6114},\n  primaryclass = {cs, stat},\n  url = {http://arxiv.org/abs/1312.6114},\n  urldate = {2021-05-29},\n  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},\n  archiveprefix = {arxiv}\n}\n\n@patent{mnihMethodsApparatusreinforcement2015,\n  title = {Methods and Apparatus for Reinforcement Learning},\n  author = {Mnih, Volodymyr and Kavukcuoglu, Koray},\n  year = {2015},\n  month = apr,\n  number = {US20150100530A1},\n  url = {https://patents.google.com/patent/US20150100530A1/en},\n  urldate = {2023-08-11},\n  assignee = {Google LLC},\n  langid = {english},\n  nationality = {US}\n}\n\n@article{rummeryOnlineQLearningusing1994,\n  title = {On-Line {{Q-Learning}} Using {{Connectionist Systms}}},\n  author = {Rummery, G A and Niranjan, M},\n  year = {1994},\n  month = sep,\n  journal = {CUED/F-INFENG/TR 166},\n  langid = {english}\n}\n\n@misc{veenNeuralNetworkZoo2016,\n  title = {The {{Neural Network Zoo}}},\n  author = {van Veen, Fjodor},\n  year = {2016},\n  month = sep,\n  journal = {The Asimov Institute},\n  url = {https://www.asimovinstitute.org/neural-network-zoo/},\n  urldate = {2021-05-29},\n  abstract = {With new neural network~architectures popping up every now and then, it\'s hard to keep track of them all. Knowing all the abbreviations being thrown around (DCIGN, BiLSTM, DCGAN, anyone?) can be a bit overwhelming at first. So I decided to compose a cheat sheet containing~many of those~architectures. Most of these~are neural networks, some are completely [{\\dots}]},\n  langid = {american}\n}\n'},51611:(e,n,a)=>{e.exports=a.p+"img/ml_nn-approaches-no-text.384a568c.png"}}]);