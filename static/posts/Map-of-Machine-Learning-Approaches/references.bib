@article{10.1037/h0042519,
  title = {The Perceptron: {{A Probabilistic Model}} for {{Information Storage}} and {{Organization}} in the {{Brain}}},
  shorttitle = {The Perceptron},
  author = {Rosenblatt, Frank},
  year = {1958},
  journal = {Psychological Review},
  volume = {65},
  number = {6},
  pages = {386--408},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/h0042519},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/h0042519},
  urldate = {2023-08-11},
  langid = {english}
}

@article{10.1109/5.58325,
  title = {The {{Self-Organizing Map}}},
  author = {Kohonen, T.},
  year = {1990},
  month = sep,
  journal = {Proceedings of the IEEE},
  volume = {78},
  number = {9},
  pages = {1464--1480},
  issn = {1558-2256},
  doi = {10.1109/5.58325},
  abstract = {The self-organized map, an architecture suggested for artificial neural networks, is explained by presenting simulation experiments and practical applications. The self-organizing map has the property of effectively creating spatially organized internal representations of various features of input signals and their abstractions. One result of this is that the self-organization process can discover semantic relationships in sentences. Brain maps, semantic maps, and early work on competitive learning are reviewed. The self-organizing map algorithm (an algorithm which order responses spatially) is reviewed, focusing on best matching cell selection and adaptation of the weight vectors. Suggestions for applying the self-organizing map algorithm, demonstrations of the ordering process, and an example of hierarchical clustering of data are presented. Fine tuning the map by learning vector quantization is addressed. The use of self-organized maps in practical speech recognition and a simulation experiment on semantic mapping are discussed.{$<>$}}
}

@article{10.1109/5.726791,
  title = {Gradient-Based Learning Applied to Document Recognition},
  author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  year = {1998},
  month = nov,
  journal = {Proceedings of the IEEE},
  volume = {86},
  number = {11},
  pages = {2278--2324},
  issn = {1558-2256},
  doi = {10.1109/5.726791},
  abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.}
}

@article{10.1162/neco.1997.9.8.1735,
  title = {Long {{Short-Term Memory}}},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  year = {1997},
  month = nov,
  journal = {Neural Computation},
  volume = {9},
  number = {8},
  pages = {1735--1780},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.1997.9.8.1735},
  url = {https://direct.mit.edu/neco/article/9/8/1735-1780/6109},
  urldate = {2021-05-29},
  abstract = {Learning to store information over extended time intervals via recurrent backpropagation takes a very long time, mostly due to insu cient, decaying error back ow. We brie y review Hochreiter's 1991 analysis of this problem, then address it by introducing a novel, e cient, gradient-based method called {\textbackslash}Long Short-Term Memory" (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete time steps by enforcing constant error ow through {\textbackslash}constant error carrousels" within special units. Multiplicative gate units learn to open and close access to the constant error ow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with arti cial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with RTRL, BPTT, Recurrent Cascade-Correlation, Elman nets, and Neural Sequence Chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, arti cial long time lag tasks that have never been solved by previous recurrent network algorithms.},
  langid = {english}
}

@article{bourlardAutoassociationMultilayerperceptrons2004,
  title = {Auto-Association by Multilayer Perceptrons and Singular Value Decomposition},
  author = {Bourlard, H. and Kamp, Y.},
  year = {2004},
  journal = {undefined},
  url = {/paper/Auto-association-by-multilayer-perceptrons-and-Bourlard-Kamp/f5821548720901c89b3b7481f7500d7cd64e99bd},
  urldate = {2021-05-29},
  abstract = {The multilayer perceptron, when working in auto-association mode, is sometimes considered as an interesting candidate to perform data compression or dimensionality reduction of the feature space in information processing applications. The present paper shows that, for auto-association, the nonlinearities of the hidden units are useless and that the optimal parameter values can be derived directly by purely linear techniques relying on singular value decomposition and low rank matrix approximation, similar in spirit to the well-known Karhunen-Lo{\`e}ve transform. This approach appears thus as an efficient alternative to the general error back-propagation algorithm commonly used for training multilayer perceptrons. Moreover, it also gives a clear interpretation of the r{\^o}le of the different parameters.},
  langid = {english}
}

@article{DBLP:journals/cogsci/Elman90,
  title = {Finding Structure in Time},
  author = {Elman, Jeffrey L.},
  year = {1990},
  journal = {Cogn. Sci.},
  volume = {14},
  number = {2},
  pages = {179--211},
  doi = {10.1207/s15516709cog1402\_1},
  url = {https://doi.org/10.1207/s15516709cog1402_1},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/journals/cogsci/Elman90.bib},
  timestamp = {Thu, 04 Jun 2020 19:37:03 +0200}
}

@article{goodfellowGenerativeAdversarialNetworks2014,
  title = {Generative {{Adversarial Networks}}},
  author = {Goodfellow, Ian J. and {Pouget-Abadie}, Jean and Mirza, Mehdi and Xu, Bing and {Warde-Farley}, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  year = {2014},
  month = jun,
  journal = {arXiv:1406.2661 [cs, stat]},
  eprint = {1406.2661},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1406.2661},
  urldate = {2021-05-29},
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  archiveprefix = {arxiv}
}

@article{kingmaAutoEncodingVariationalBayes2014,
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, Diederik P. and Welling, Max},
  year = {2014},
  month = may,
  journal = {arXiv:1312.6114 [cs, stat]},
  eprint = {1312.6114},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1312.6114},
  urldate = {2021-05-29},
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  archiveprefix = {arxiv}
}

@patent{mnihMethodsApparatusreinforcement2015,
  title = {Methods and Apparatus for Reinforcement Learning},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray},
  year = {2015},
  month = apr,
  number = {US20150100530A1},
  url = {https://patents.google.com/patent/US20150100530A1/en},
  urldate = {2023-08-11},
  assignee = {Google LLC},
  langid = {english},
  nationality = {US}
}

@article{rummeryOnlineQLearningusing1994,
  title = {On-Line {{Q-Learning}} Using {{Connectionist Systms}}},
  author = {Rummery, G A and Niranjan, M},
  year = {1994},
  month = sep,
  journal = {CUED/F-INFENG/TR 166},
  langid = {english}
}

@misc{veenNeuralNetworkZoo2016,
  title = {The {{Neural Network Zoo}}},
  author = {van Veen, Fjodor},
  year = {2016},
  month = sep,
  journal = {The Asimov Institute},
  url = {https://www.asimovinstitute.org/neural-network-zoo/},
  urldate = {2021-05-29},
  abstract = {With new neural network~architectures popping up every now and then, it's hard to keep track of them all. Knowing all the abbreviations being thrown around (DCIGN, BiLSTM, DCGAN, anyone?) can be a bit overwhelming at first. So I decided to compose a cheat sheet containing~many of those~architectures. Most of these~are neural networks, some are completely [{\dots}]},
  langid = {american}
}
